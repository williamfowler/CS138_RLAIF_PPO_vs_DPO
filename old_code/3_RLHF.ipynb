{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kQRguV413i9"
      },
      "source": [
        "# RLHF Fine-Tuning\n",
        "\n",
        "## Mount Google Drive\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_2QMYbLqVIV",
        "outputId": "61d61523-5215-4b4a-802d-471b41ecc63d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/datasets'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlDcOKH3rClU",
        "outputId": "63a2f789-41ac-44aa-8d47-ed6abea05aeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt_formatted_dataset_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1kH4xVflCe_",
        "outputId": "77674119-1a35-496f-ed9a-bfa5ef02db8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEy3u4dV2BPc"
      },
      "source": [
        "# Mount Google Drive to access dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKIA80F2NK_"
      },
      "source": [
        "## LLM Judge (Reward Function)\n",
        "\n",
        "Instead of using a trained reward model, we'll use an LLM API (like GPT-4 or Claude) to score the quality of generated responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0g0VIlFvUgLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "160b126a-c294-41d6-d090-29313d4cedb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test LLM Judge Score: -0.4600\n",
            "✓ LLM judge function ready (using placeholder - replace with real API later)\n"
          ]
        }
      ],
      "source": [
        "def get_llm_judge_score(prompt, response):\n",
        "    \"\"\"\n",
        "    Get a reward score from an LLM judge for a given prompt-response pair.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The original prompt/question\n",
        "        response (str): The model's generated response\n",
        "\n",
        "    Returns:\n",
        "        float: Reward score in range [-1, 1]\n",
        "    \"\"\"\n",
        "    # TODO: Replace this with actual API call to GPT-4, Claude, or other LLM\n",
        "    # Example API call structure:\n",
        "    # judge_prompt = f\"Rate the quality of this response (0-10):\\nQuestion: {prompt}\\nAnswer: {response}\\nRating:\"\n",
        "    # api_response = call_llm_api(judge_prompt)\n",
        "    # score = extract_score(api_response) / 5.0 - 1.0  # Normalize to [-1, 1]\n",
        "\n",
        "    # PLACEHOLDER: Mock scoring based on response length\n",
        "    # Longer responses get slightly higher scores (just for testing)\n",
        "    mock_score = min(len(response) / 200.0, 1.0)  # 0 to 1\n",
        "    mock_score = 2 * (mock_score - 0.5)  # Convert to [-1, 1] range\n",
        "\n",
        "    return float(mock_score)\n",
        "\n",
        "# Test the placeholder function\n",
        "test_prompt = \"What is the capital of France?\"\n",
        "test_response = \"Paris is the capital and most populous city of France.\"\n",
        "test_score = get_llm_judge_score(test_prompt, test_response)\n",
        "print(f\"Test LLM Judge Score: {test_score:.4f}\")\n",
        "print(\"✓ LLM judge function ready (using placeholder - replace with real API later)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbFZryb3Um49",
        "outputId": "28f28144-578d-4cb2-9ca0-82dc14139bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward model loading skipped - will use LLM judge API instead\n"
          ]
        }
      ],
      "source": [
        "# OLD: Load reward model from file\n",
        "# model_name = \"gpt2\"\n",
        "# reward_model = GPT2RewardModel(model_name)\n",
        "# reward_model.load_state_dict(torch.load(\"reward_model.pt\", map_location='cpu'))\n",
        "\n",
        "print(\"Reward model loading skipped - will use LLM judge API instead\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juFUbSNfVn2U"
      },
      "source": [
        "## Policy Model with Value Head\n",
        "\n",
        "PPO requires two components:\n",
        "- **Policy (Actor)**: The language model that generates text\n",
        "- **Value Head (Critic)**: Estimates the expected reward for a given state\n",
        "\n",
        "We combine both into a single model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sQo4pbqGVv3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dcc500-d055-41a0-bfa9-06760e3e245f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "from typing import Optional\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "class ValueHead(nn.Module):\n",
        "    \"\"\"\n",
        "    The ValueHead class implements a head for the model\n",
        "    that returns a scalar for each output token.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.value = nn.Linear(self.hidden_size, 1)\n",
        "        self._post_init()\n",
        "\n",
        "    def _post_init(self):\n",
        "        nn.init.normal_(self.value.weight, std=(1.0 / np.sqrt(self.hidden_size + 1)))\n",
        "        nn.init.zeros_(self.value.bias)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        output = hidden_states\n",
        "        return self.value(output)\n",
        "\n",
        "\n",
        "class ModelForCausalLMWithValueHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Causal LM model with a value head on top.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name_or_path, quantization_config=None):\n",
        "        super().__init__()\n",
        "        # NEW: Support loading from HuggingFace with quantization\n",
        "        if quantization_config is not None:\n",
        "            self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16,\n",
        "            )\n",
        "        else:\n",
        "            # OLD: Load from local path (GPT-2)\n",
        "            self.llm = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "\n",
        "        # Add the value head\n",
        "        self.v_head = ValueHead(self.llm.config)\n",
        "\n",
        "        # IMPORTANT: Move value head to same device as the LLM\n",
        "        # With device_map=\"auto\", the LLM is on GPU but v_head might be on CPU\n",
        "        if quantization_config is not None:\n",
        "            # Find which device the LLM is on\n",
        "            try:\n",
        "                llm_device = next(self.llm.parameters()).device\n",
        "                self.v_head = self.v_head.to(llm_device)\n",
        "                print(f\"Value head moved to {llm_device}\")\n",
        "            except StopIteration:\n",
        "                print(\"Warning: Could not determine LLM device\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "    ) -> Optional[torch.FloatTensor]:\n",
        "\n",
        "        transformer_outputs = self.llm.forward(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states = True,\n",
        "        )\n",
        "        lm_logits = transformer_outputs.logits\n",
        "        # Get the last hidden state\n",
        "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
        "\n",
        "        # Apply the value head\n",
        "        value = self.v_head(last_hidden_state).squeeze(-1)\n",
        "        return lm_logits, value\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.llm.generate(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X3xGmHI8W0Q0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "ad2d2ef1bab84bbb994081448f5e24bf",
            "d116cf3fc523425b85d777bed99ca654",
            "8f1a06c2b144420aa844b1afd9562b36",
            "d3343f64aad84751bd9d44fbc6ff2b35",
            "c13404bfe14344d781a863559a6b4b3c",
            "0de6966bd48e490da467c7314f62c48b",
            "28266d13f1da49a7b14e8dd28ad4438f",
            "0d0bb0909eab424d840c18848e684099",
            "e3a41b26942f4d7a9b060b313f2eb31f",
            "703ef3fd72a043529769fad9351d1328",
            "112c1793e9e8471fbd58d2b9ff888a8d"
          ]
        },
        "outputId": "a4ffcc51-c449-4207-f35a-55fd6d038927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad2d2ef1bab84bbb994081448f5e24bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value head moved to cuda:0\n",
            "Zephyr 7B model loaded successfully with value head!\n"
          ]
        }
      ],
      "source": [
        "# OLD: Load GPT-2 from local path\n",
        "# model_path = './sft_model_epoch_1'\n",
        "# model = ModelForCausalLMWithValueHead(model_path)\n",
        "\n",
        "# NEW: Load Zephyr 7B from HuggingFace with 4-bit quantization\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Quantization config for 4-bit loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model with value head\n",
        "model = ModelForCausalLMWithValueHead(model_name, quantization_config=bnb_config)\n",
        "print(\"Zephyr 7B model loaded successfully with value head!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqUTbr_DXA0j"
      },
      "source": [
        "## Preparing Dataset\n",
        "\n",
        "We load prompts from our CSV file and tokenize them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jpFxrOLgXTkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f5fcb0-fb32-4dd0-d37b-5adccd14f915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded. Pad token: </s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# OLD: Load GPT-2 tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# NEW: Load Zephyr tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Important for batch generation\n",
        "print(f\"Tokenizer loaded. Pad token: {tokenizer.pad_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NeWPFknOXf_2",
        "outputId": "e9c6beda-767b-4082-a348-9befadfa1eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dummy dataset created at datasets/gpt_formatted_dataset_clean.csv\n",
            "✓ Contains 8 sample prompts for testing\n",
            "✓ Prompts will be formatted with Zephyr chat template during tokenization\n"
          ]
        }
      ],
      "source": [
        "# TEMPORARY: Create dummy dataset for testing (remove when using real dataset from Drive)\n",
        "import os\n",
        "os.makedirs('datasets', exist_ok=True)\n",
        "\n",
        "# NOTE: For Zephyr, we need to format prompts using the chat template\n",
        "# The tokenizer has a built-in apply_chat_template() method\n",
        "# We'll store just the user questions in the CSV and format them later during tokenization\n",
        "\n",
        "dummy_csv = \"\"\"prompt,chosen,rejected\n",
        "Explain why the sky looks blue.,\"Answer: The sky appears blue because molecules in the atmosphere scatter shorter wavelengths of sunlight more efficiently than longer ones.\",\"The sky looks blue due to Rayleigh scattering.\"\n",
        "How does photosynthesis work in plants?,\"Answer: Photosynthesis uses light, water, and carbon dioxide to produce glucose and oxygen in chloroplasts.\",\"Photosynthesis is the way plants turn light into food.\"\n",
        "What is the capital city of Japan?,Answer: Tokyo is the capital city of Japan.,Tokyo is the capital city of Japan.\n",
        "Describe the function of the human heart.,Answer: The heart pumps oxygenated and deoxygenated blood through the body to support cellular activity.,The heart moves blood throughout the body.\n",
        "Why do objects fall toward Earth?,\"Answer: Objects fall because gravity pulls masses toward each other, with Earth exerting a strong attractive force.\",Objects fall since gravity pulls them down.\n",
        "What is a prime number?,Answer: A prime number is an integer greater than one that has no positive divisors other than one and itself.,A prime number is a number divisible only by one and itself.\n",
        "Explain the water cycle.,\"Answer: The water cycle involves evaporation, condensation, precipitation, and collection as water moves through Earth systems.\",\"The water cycle is the movement of water through evaporation, condensation, and precipitation.\"\n",
        "How do vaccines help protect people?,Answer: Vaccines stimulate the immune system to recognize specific pathogens so the body can respond quickly if exposed.,Vaccines train the immune system to identify harmful microbes.\n",
        "\"\"\"\n",
        "\n",
        "with open('datasets/gpt_formatted_dataset_clean.csv', 'w') as f:\n",
        "    f.write(dummy_csv)\n",
        "\n",
        "print(\"✓ Dummy dataset created at datasets/gpt_formatted_dataset_clean.csv\")\n",
        "print(\"✓ Contains 8 sample prompts for testing\")\n",
        "print(\"✓ Prompts will be formatted with Zephyr chat template during tokenization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y32xsqLokOKi",
        "outputId": "249cefeb-4307-4e1f-ca07-8c75e25442f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 307 prompts\n",
            "Columns: ['prompt', 'chosen', 'rejected']\n",
            "\n",
            "Sample prompt: Explain why the sky looks blue.\n"
          ]
        }
      ],
      "source": [
        "# OLD: Load SST2 sentiment dataset\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"sst2\")\n",
        "# dataset\n",
        "\n",
        "# NEW: Load our prompt dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# TEMPORARY: Using local dummy dataset for testing in VS Code\n",
        "# WHEN READY FOR COLAB: Replace with Google Drive path:\n",
        "dataset_path = \"/content/drive/MyDrive/datasets/gpt_formatted_dataset_clean.csv\"\n",
        "# dataset_path = \"datasets/gpt_formatted_dataset_clean.csv\"\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=dataset_path)\n",
        "print(f\"Dataset loaded: {len(dataset['train'])} prompts\")\n",
        "print(f\"Columns: {dataset['train'].column_names}\")\n",
        "print(f\"\\nSample prompt: {dataset['train'][0]['prompt']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r8r6wI0ZXpqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544f9ed4-d5c3-4424-8f0e-8b1fd107cec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 276\n",
            "Val size: 31\n"
          ]
        }
      ],
      "source": [
        "# OLD: Split SST2 dataset\n",
        "# ds_train, ds_val = dataset['train'], dataset['validation']\n",
        "\n",
        "# NEW: Create train/val split from our dataset\n",
        "train_size = int(0.9 * len(dataset['train']))\n",
        "ds_train = dataset['train'].select(range(train_size))\n",
        "ds_val = dataset['train'].select(range(train_size, len(dataset['train'])))\n",
        "\n",
        "print(f\"Train size: {len(ds_train)}\")\n",
        "print(f\"Val size: {len(ds_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Krfsh5Xwq5"
      },
      "source": [
        "# OLD: Filtering - not needed for our dataset\n",
        "print(\"Filtering section skipped - our dataset already has suitable prompts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0gae7XcX9ri",
        "outputId": "5b02177e-eda1-4b57-9dc7-b8994d3c16c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 276\n"
          ]
        }
      ],
      "source": [
        "# OLD: Check length after filtering\n",
        "# len(ds_train)\n",
        "\n",
        "print(f\"Train size: {len(ds_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MXq59PWPX_CC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25584d4-31e7-4c77-c29d-6ebad0b4c03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No additional filtering needed for our prompts\n"
          ]
        }
      ],
      "source": [
        "# OLD: Filter validation for SST2\n",
        "# ds_train = ds_train.filter(lambda x: len(x['sentence'].split(' ')) > 8)\n",
        "\n",
        "# NEW: No filtering needed\n",
        "print(\"No additional filtering needed for our prompts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyt4_oCIYRO6",
        "outputId": "bd3d2b4e-18bc-4da7-c47b-6d3b170c284b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 276\n"
          ]
        }
      ],
      "source": [
        "# OLD: Check length after filtering\n",
        "# len(ds_train)\n",
        "\n",
        "print(f\"Train size: {len(ds_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D--0rAUSYTaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b140324-c101-4970-abd9-568d3008dfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No additional filtering needed for validation set\n"
          ]
        }
      ],
      "source": [
        "# OLD: Filter validation for SST2\n",
        "# ds_val = ds_val.filter(lambda x: len(x['sentence'].split(' ')) > 8)\n",
        "\n",
        "# NEW: No filtering needed\n",
        "print(\"No additional filtering needed for validation set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuTJnI0VYXbl",
        "outputId": "6d431118-46bb-4129-9b12-1582aa5a1084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val size: 31\n"
          ]
        }
      ],
      "source": [
        "# OLD: Check val length\n",
        "# len(ds_val)\n",
        "\n",
        "print(f\"Val size: {len(ds_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVO2_DDEYYif",
        "outputId": "f8a81582-ded8-423a-fccf-bc66a29734a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using full prompts from dataset (no truncation)\n"
          ]
        }
      ],
      "source": [
        "# OLD: Random input token length for SST2 truncation\n",
        "# import random\n",
        "# input_min_token_length = 2\n",
        "# input_max_token_length = 8\n",
        "# input_token_length_range = list(range(input_min_token_length, input_max_token_length))\n",
        "# print(input_token_length_range)\n",
        "\n",
        "# NEW: We'll use full prompts from our dataset (no truncation)\n",
        "import random\n",
        "print(\"Using full prompts from dataset (no truncation)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VD3Rny7YvxX",
        "outputId": "53aa0ff2-f06e-49f3-a381-aa90a3a28dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random truncation not used - using full prompts\n"
          ]
        }
      ],
      "source": [
        "# OLD: Test random choice\n",
        "# random.choice(input_token_length_range)\n",
        "\n",
        "print(\"Random truncation not used - using full prompts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iB0Msyo7Y6B5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "2eda750751f249dcbe38d0e19512e161",
            "3b43cd6d80fc427ba5109cf1a5f2f8cd",
            "1b5e6ff2c510437e97aff10741dfd9e2",
            "dc02a64433b14be49d5911efc1e0348b",
            "8ace145730144e509135f830df765a38",
            "1d0625677c164439bafb5b0b30f6e2b9",
            "2c826ad713ec4b5ba0fe8452910aa1e1",
            "cd0a4885d80c4b499b07e2505d699a4f",
            "4ef23ae50969464e89281e4c258dafe8",
            "2df22dd37cba40abb3bc418107567c7a",
            "30a431d060f949dca7e96f2a597497a7",
            "cfc2bdf9a8d74e49b4688742473cbc8f",
            "24322f82677c4fbd831d486e94a24637",
            "aca6ce3f772e40139e4e4466c0b21c85",
            "cb50225678a64c6fa1338a52b77e6582",
            "c3dcf3f5c9b44751840d5b87c4b86f22",
            "78fff1719f0b4873aebca1f2010700e0",
            "eca8a5b096d14c468e91610a78fbe9f3",
            "c19278d72fc3475e98bd7ce8c1d64afb",
            "4f75118989ae42cb9dcb9b82bdf023fd",
            "66ec28a2119440149fafae0f1675bc54",
            "88004abd7de34ed6aa02b79c75f46ce7"
          ]
        },
        "outputId": "c5cca9b1-0085-49ec-9fc0-7268eeb3f5ba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/276 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eda750751f249dcbe38d0e19512e161"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfc2bdf9a8d74e49b4688742473cbc8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized 276 training prompts\n",
            "Tokenized 31 validation prompts\n",
            "\n",
            "Sample formatted prompt:\n",
            "'<|user|>\\nExplain why the sky looks blue.</s>\\n<|assistant|>\\n'\n",
            "\n",
            "Token IDs (first 20): [523, 28766, 1838, 28766, 28767, 13, 966, 19457, 2079, 272, 7212, 4674, 5045, 28723, 2, 28705, 13, 28789, 28766, 489]\n",
            "Number of tokens: 24\n"
          ]
        }
      ],
      "source": [
        "# OLD: Tokenize with random truncation for SST2\n",
        "# def tokenize(sample):\n",
        "#     input_size = random.choice(input_token_length_range)\n",
        "#     sample['input_ids'] = tokenizer.encode(sample['sentence'])[:input_size]\n",
        "#     sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
        "#     sample['query'] = tokenizer.decode(sample['input_ids'])\n",
        "#     return sample\n",
        "\n",
        "# NEW: Tokenize prompts using Zephyr's chat template\n",
        "def tokenize(sample):\n",
        "    # Format the prompt using Zephyr's chat template\n",
        "    # Zephyr expects: <|user|>\\n{prompt}</s>\\n<|assistant|>\\n\n",
        "\n",
        "    try:\n",
        "        # Try using the built-in chat template\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": sample['prompt']}\n",
        "        ]\n",
        "\n",
        "        # Use tokenize=True to get token IDs directly\n",
        "        encoded = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=None  # Return list, not tensor\n",
        "        )\n",
        "\n",
        "        # Get the formatted text for debugging\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Fallback: Manually format using Zephyr's template\n",
        "        # Based on: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
        "        formatted_prompt = f\"<|user|>\\n{sample['prompt']}</s>\\n<|assistant|>\\n\"\n",
        "        # Use add_special_tokens=True to properly handle special tokens\n",
        "        encoded = tokenizer.encode(formatted_prompt, add_special_tokens=True)\n",
        "\n",
        "    sample['input_ids'] = encoded\n",
        "    sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
        "    sample['query'] = formatted_prompt  # Keep formatted prompt text\n",
        "    return sample\n",
        "\n",
        "map_kwargs = {\n",
        "    \"batched\": False,\n",
        "    \"remove_columns\": ['prompt', 'chosen', 'rejected']  # Remove CSV columns, keep tokenized data\n",
        "}\n",
        "\n",
        "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
        "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_dataset_train)} training prompts\")\n",
        "print(f\"Tokenized {len(tokenized_dataset_val)} validation prompts\")\n",
        "print(f\"\\nSample formatted prompt:\")\n",
        "print(repr(tokenized_dataset_train[0]['query']))\n",
        "print(f\"\\nToken IDs (first 20): {tokenized_dataset_train[0]['input_ids'][:20]}\")\n",
        "print(f\"Number of tokens: {len(tokenized_dataset_train[0]['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N3udCtXkOKt",
        "outputId": "cab033b0-1fe7-4edc-845c-cba368890149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Has chat_template: True\n",
            "Chat template: {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "\n",
            "Formatted: '<|user|>\\nHello</s>\\n<|assistant|>\\n'\n",
            "Tokens: [523, 28766, 1838, 28766, 28767, 13, 16230, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13]\n",
            "Max token ID: 28789\n",
            "Vocab size: 32000\n"
          ]
        }
      ],
      "source": [
        "# Check if tokenizer has chat template\n",
        "print(\"Has chat_template:\", hasattr(tokenizer, 'chat_template'))\n",
        "print(\"Chat template:\", tokenizer.chat_template if hasattr(tokenizer, 'chat_template') else \"None\")\n",
        "\n",
        "# Test with simple example\n",
        "test_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
        "try:\n",
        "    result = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "    print(\"\\nFormatted:\", repr(result))\n",
        "\n",
        "    tokens = tokenizer.apply_chat_template(test_messages, tokenize=True, add_generation_prompt=True)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Max token ID:\", max(tokens))\n",
        "    print(\"Vocab size:\", tokenizer.vocab_size)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dmragNXwZxEz"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset_train.set_format(type='torch')\n",
        "tokenized_dataset_val.set_format(type='torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_zwH0J2Z__M",
        "outputId": "2ed21356-2880-4cff-8a7d-84a8acf20a6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  523, 28766,  1838, 28766, 28767,    13,   966, 19457,   272,  2130,\n",
              "         10061, 28723,     2, 28705,    13, 28789, 28766,   489, 11143, 28766,\n",
              "         28767,    13]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'query': '<|user|>\\nExplain the water cycle.</s>\\n<|assistant|>\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenized_dataset_train[6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxG13E1haCh7"
      },
      "source": [
        "## Reward Token\n",
        "\n",
        "The reward token marks where we compute the final reward score for a generated sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XqjWwhamIGH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a726b6-f2ef-412b-b985-140c7e507e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders created with batch_size=4\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# NOTE: Batch size may need to be reduced for Zephyr 7B (larger than GPT-2)\n",
        "# Start with small batch size and increase if memory allows\n",
        "batch_size = 4  # Reduced from 32 for 7B model\n",
        "\n",
        "def collator(batch):\n",
        "    return dict((key, [d[key] for d in batch]) for key in batch[0])\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset_train, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
        "val_dataloader = DataLoader(tokenized_dataset_val, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
        "\n",
        "print(f\"Dataloaders created with batch_size={batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uCy92clTKgl7",
        "outputId": "452a8991-a2fd-4f82-8a0b-4aac8bad8075"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [tensor([  523, 28766,  1838, 28766, 28767,    13,  5660,   511,  9923,   625,\n",
              "           3408, 28804,     2, 28705,    13, 28789, 28766,   489, 11143, 28766,\n",
              "          28767,    13]),\n",
              "  tensor([  523, 28766,  1838, 28766, 28767,    13,  7638,   349,  5168, 11049,\n",
              "           5857, 28804,     2, 28705,    13, 28789, 28766,   489, 11143, 28766,\n",
              "          28767,    13]),\n",
              "  tensor([  523, 28766,  1838, 28766, 28767,    13,  5660,  1235,   264, 11594,\n",
              "          11598,  5266,  7641, 28804,     2, 28705,    13, 28789, 28766,   489,\n",
              "          11143, 28766, 28767,    13]),\n",
              "  tensor([  523, 28766,  1838, 28766, 28767,    13,  7638,   349, 22950,  2278,\n",
              "            354, 10589, 28804,     2, 28705,    13, 28789, 28766,   489, 11143,\n",
              "          28766, 28767,    13])],\n",
              " 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n",
              " 'query': ['<|user|>\\nHow do plants get energy?</s>\\n<|assistant|>\\n',\n",
              "  '<|user|>\\nWhy is learning math useful?</s>\\n<|assistant|>\\n',\n",
              "  '<|user|>\\nHow does a thermometer measure temperature?</s>\\n<|assistant|>\\n',\n",
              "  '<|user|>\\nWhy is sunlight important for humans?</s>\\n<|assistant|>\\n']}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "W23GG8TLKv43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60d19ad-3d82-467d-8b66-78f29f8f1fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation settings: 20-100 tokens\n"
          ]
        }
      ],
      "source": [
        "# Generation settings for model responses\n",
        "# NOTE: These values control response length - adjust based on your needs\n",
        "output_min_length = 20  # Increased from 5 for Zephyr\n",
        "output_max_length = 100  # Increased from 16 for more complete responses\n",
        "\n",
        "# https://huggingface.co/docs/trl/how_to_train#how-to-generate-text-for-training\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 0,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id\n",
        "}\n",
        "\n",
        "print(f\"Generation settings: {output_min_length}-{output_max_length} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcIGNP_Zh05i"
      },
      "source": [
        "## Sample Generation (Test)\n",
        "\n",
        "Let's test that our model can generate responses before starting training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJrexC5oh2W8",
        "outputId": "7fa5a0fa-948b-4203-8c59-4068298a0e24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [1, 15359, 28725, 456], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "sample = tokenizer('Hi, this')\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afkv6jkjiJgp",
        "outputId": "fc27bd19-da25-4683-e973-acc8e305d7ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    1, 15359, 28725,   456,   349, 12642, 28725,   574,   413,  6255,\n",
              "        28459, 28723,   315,   682,   737,   298,  1985,   395,   368,   684,\n",
              "          413,  6255,  3154, 28723,    13,    13,  1014,  1183,  1689,  1082,\n",
              "          413,  6255, 10969,   354,  4300,   390,   264,  7052, 15589, 28723,\n",
              "          413,  6687,   349,   264,  5168,  1759,   354,   905,   354,  6105,\n",
              "         4300,   349,   459,   652,   907,  3842, 28723,  1047,   368,   460,\n",
              "          297,   272,  2969,  3543,   304,   927,   298,  2822,  4300, 28725,\n",
              "          368,   993,  4987,   396,   413,  6255,   875, 28725,  1096,  4300,\n",
              "          349,   579, 12588,   298,  6790,  1411, 28725], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "query_response = model.generate(\n",
        "    input_ids=torch.tensor(sample['input_ids']).unsqueeze(0).to(device),\n",
        "    attention_mask=torch.tensor(sample['attention_mask']).unsqueeze(0).to(device),\n",
        "    **generation_kwargs\n",
        "    ).squeeze(0)\n",
        "query_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "eh4EEAF6ioeQ",
        "outputId": "61c93139-c4d3-4fde-e2d9-7066ed7ef94e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Hi, this is Sarah, your ESL consultant. I would like to talk with you about ESL today.\\n\\nThe acronym ESL stands for English as a Second Language. ELS is a learning process for people for whom English is not their first language. If you are in the United States and need to learn English, you may choose an ESL class, because English is so vital to daily life,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tokenizer.decode(query_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Np4NQ_iuAX",
        "outputId": "ed8883e3-67ed-44e4-97e5-32ecd58e65be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Hi, this\n",
            "Response: Hi, this is Sarah, your ESL consultant. I would like to talk with you about ESL today.\n",
            "\n",
            "The acronym ESL stands for English as a Second Language. ELS is a learning process for people for whom English is not their first language. If you are in the United States and need to learn English, you may choose an ESL class, because English is so vital to daily life,\n",
            "LLM Judge Score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Test scoring a generated response with LLM judge\n",
        "with torch.no_grad():\n",
        "    # Decode the generated response\n",
        "    response_text = tokenizer.decode(query_response, skip_special_tokens=True)\n",
        "    prompt_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "    # Get score from LLM judge (replaces reward model)\n",
        "    score = get_llm_judge_score(prompt_text, response_text)\n",
        "\n",
        "    print(f\"Prompt: {prompt_text}\")\n",
        "    print(f\"Response: {response_text}\")\n",
        "    print(f\"LLM Judge Score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLxKzwl8jxQ_"
      },
      "source": [
        "## Batch Generation\n",
        "\n",
        "Now let's test generating responses for a full batch of prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMyH9qhQj8Fg",
        "outputId": "7db2883f-ce8f-4d90-e8fa-5f9e7cb79b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Generating responses for 4 prompts...\n",
            "<|user|>\n",
            "How do plants get energy? \n",
            "<|assistant|>\n",
            "\n",
            "<|user|>\n",
            "Why is learning math useful? \n",
            "<|assistant|>\n",
            "\n",
            "<|user|>\n",
            "How does a thermometer measure temperature? \n",
            "<|assistant|>\n",
            "\n",
            "<|user|>\n",
            "Why is sunlight important for humans? \n",
            "<|assistant|>\n",
            "\n",
            "Generated responses:\n",
            "[\"Plants get energy through a process called photosynthesis. During photosynthesis, chlorophyll (a green pigment) in the plant's cells absorbs light energy from the sun.\", 'Learning math is useful for several reasons:\\n\\n1. Problem-solving skills: Math equips us with problem-solving skills that we can apply to various situations in life. Math helps us understand and analyze patterns, identify relationships', 'A thermometer measures temperature by detecting and responding to changes in the amount of energy, known as heat, that is stored', 'Sunlight is important for humans in the following ways:\\n\\n1. Vitamin D: Sunlight is the main source of vitamin D for most people. The body synthesizes vitamin']\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# NOTE: Model already on device via device_map=\"auto\", so we don't move it\n",
        "# model = model.to(device)  # Not needed with device_map=\"auto\"\n",
        "\n",
        "query_tensors = batch['input_ids']\n",
        "query_attention_masks = batch['attention_mask']\n",
        "\n",
        "response_tensors = []\n",
        "query_response_tensors = []\n",
        "score_tensors = []\n",
        "\n",
        "print(f\"Generating responses for {len(query_tensors)} prompts...\")\n",
        "\n",
        "for i, query in enumerate(query_tensors):\n",
        "    query = query.to(device)\n",
        "    query_attention_mask = query_attention_masks[i].to(device)\n",
        "    new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "    generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "    query_response = model.generate(\n",
        "        input_ids=query.unsqueeze(0),\n",
        "        attention_mask=query_attention_mask.unsqueeze(0),\n",
        "        **generation_kwargs\n",
        "    ).squeeze(0)\n",
        "\n",
        "    response_len = len(query_response) - len(query)\n",
        "    response_tensors.append(query_response[-response_len:])\n",
        "    query_response_tensors.append(query_response)\n",
        "\n",
        "    # Use LLM judge instead of reward model\n",
        "    with torch.no_grad():\n",
        "        prompt_text = tokenizer.decode(query, skip_special_tokens=True)\n",
        "        print(prompt_text)\n",
        "        response_text = tokenizer.decode(query_response[-response_len:], skip_special_tokens=True)\n",
        "\n",
        "        # Get score from LLM judge\n",
        "        score = get_llm_judge_score(prompt_text, response_text)\n",
        "        score = torch.tensor(score).to(device)\n",
        "\n",
        "    score_tensors.append(score)\n",
        "\n",
        "batch[\"response\"] = [tokenizer.decode(response, skip_special_tokens=True) for response in response_tensors]\n",
        "print(\"Generated responses:\")\n",
        "print(batch['response'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1K4nAUG5fes"
      },
      "source": [
        "## Compute Reward\n",
        "\n",
        "The reward function combines:\n",
        "- **Score from LLM judge**: Quality of the response\n",
        "- **KL penalty**: Prevents the model from diverging too much from the reference (SFT) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-zd0KK85pGT"
      },
      "source": [
        "**Reward Formula:**\n",
        "\n",
        "$\\text{reward} = \\text{score} - \\beta \\cdot \\log \\left(\\frac{\\pi^{RL}_\\theta}{\\pi^{SFT}}\\right)$\n",
        "\n",
        "Where:\n",
        "- $\\text{score}$ is from the LLM judge\n",
        "- $\\beta$ is the KL penalty coefficient (controls how much we penalize divergence)\n",
        "- $\\pi^{RL}_\\theta$ is the current policy (model being trained)\n",
        "- $\\pi^{SFT}$ is the reference policy (frozen SFT model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jrF8ybakOK5",
        "outputId": "a9201279-9e63-4fec-b85a-3a266d18a63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value head device: cuda:0\n",
            "Value head dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "# Adding these so that types match up correctly\n",
        "\n",
        "# Move the value head to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.v_head.to(device)\n",
        "\n",
        "# Verify it works\n",
        "print(f\"Value head device: {next(model.v_head.parameters()).device}\")\n",
        "\n",
        "# Convert the value head to float16 to match the base model's output\n",
        "model.v_head.to(dtype=torch.bfloat16)\n",
        "\n",
        "# Verify the fix\n",
        "print(f\"Value head dtype: {model.v_head.value.weight.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aW8X18-Mlb3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0e1ea3-f9f4-443f-d4f9-a465b40a46b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating reference model (this may take a moment for 7B model)...\n",
            "✓ Reference model created\n"
          ]
        }
      ],
      "source": [
        "# Create reference model (frozen copy for KL divergence)\n",
        "# NOTE: For large models like Zephyr 7B, deepcopy might use a lot of memory\n",
        "# The reference model stays frozen during training\n",
        "from copy import deepcopy\n",
        "print(\"Creating reference model (this may take a moment for 7B model)...\")\n",
        "sft_model = deepcopy(model)\n",
        "sft_model.eval()  # Set to evaluation mode (frozen)\n",
        "print(\"✓ Reference model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sFAE8zQPT-uP"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hi6IhpByUWWq",
        "outputId": "c035284e-ca94-4afe-8087-7455ebba4aa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,   523, 28766,  1838,\n",
              "         28766, 28767,    13,  5660,   511,  9923,   625,  3408, 28804,     2,\n",
              "         28705,    13, 28789, 28766,   489, 11143, 28766, 28767,    13,  2249,\n",
              "          1549,   625,  3408,  1059,   264,  1759,  1987,  8886, 28724,   448,\n",
              "         21537, 28723,  6213,  8886, 28724,   448, 21537, 28725,   484,  5638,\n",
              "          3126, 19530,   325, 28708,  5344, 18958,   466, 28731,   297,   272,\n",
              "          5100, 28742, 28713,  8894, 10612,  1816,  2061,  3408,   477,   272,\n",
              "          4376, 28723],\n",
              "        [  523, 28766,  1838, 28766, 28767,    13,  7638,   349,  5168, 11049,\n",
              "          5857, 28804,     2, 28705,    13, 28789, 28766,   489, 11143, 28766,\n",
              "         28767,    13, 28758,   644,   971, 11049,   349,  5857,   354,  2856,\n",
              "          6494, 28747,    13,    13, 28740, 28723, 24857, 28733, 28713, 18390,\n",
              "          6266, 28747,  6960,  1734,  2430,   592,   395,  2700, 28733, 28713,\n",
              "         18390,  6266,   369,   478,   541,  5580,   298,  4118, 11846,   297,\n",
              "          1411, 28723,  6960,  7263,   592,  2380,   304, 20765, 11533, 28725,\n",
              "          9051,  9391],\n",
              "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,   523, 28766,  1838, 28766, 28767,    13,  5660,\n",
              "          1235,   264, 11594, 11598,  5266,  7641, 28804,     2, 28705,    13,\n",
              "         28789, 28766,   489, 11143, 28766, 28767,    13, 28741, 11594, 11598,\n",
              "         10582,  7641,   486,  6705,   288,   304, 26167,   298,  4435,   297,\n",
              "           272,  3558,   302,  3408, 28725,  2651,   390,  6601, 28725,   369,\n",
              "           349, 11141],\n",
              "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,   523,\n",
              "         28766,  1838, 28766, 28767,    13,  7638,   349, 22950,  2278,   354,\n",
              "         10589, 28804,     2, 28705,    13, 28789, 28766,   489, 11143, 28766,\n",
              "         28767,    13, 28524,  3646,   349,  2278,   354, 10589,   297,   272,\n",
              "          2296,  4342, 28747,    13,    13, 28740, 28723, 20874,  8255,   384,\n",
              "         28747,  7057,  3646,   349,   272,  2191,  2832,   302, 11781,  8255,\n",
              "           384,   354,  1080,   905, 28723,   415,  2187, 13606,  2053,  5004,\n",
              "         11781,  8255]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "       device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "input_data = data_collator([\n",
        "    {'input_ids': ids,\n",
        "     'attention_mask': torch.ones_like(ids)} for ids in query_response_tensors\n",
        "]).to(device)\n",
        "input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FxbbEH4eX4Ab"
      },
      "outputs": [],
      "source": [
        "def compute_rewards(input_data, query_tensors, response_tensors, score_tensors):\n",
        "    with torch.no_grad():\n",
        "        logits, values = model(**input_data) # b, seq, vocab\n",
        "        ref_logits, _ = sft_model(**input_data)\n",
        "        logp = torch.nn.functional.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "        ref_logp = torch.nn.functional.log_softmax(ref_logits[:, :-1, :], dim=-1)\n",
        "\n",
        "        labels = input_data['input_ids'][:, 1:] # b, seq\n",
        "\n",
        "        logp = torch.gather(logp, 2, labels.unsqueeze(-1)).squeeze(-1) # batch, seq\n",
        "        ref_logp = torch.gather(ref_logp, 2, labels.unsqueeze(-1)).squeeze(-1) # batch, seq\n",
        "\n",
        "        kl = logp - ref_logp\n",
        "        beta = 0.2\n",
        "        rewards = - beta * kl\n",
        "        attention_mask = input_data['attention_mask']\n",
        "        masks = torch.zeros_like(attention_mask[:, 1:])\n",
        "        masks[:,:] = attention_mask[:, 1:]\n",
        "        for j in range(len(query_tensors)):\n",
        "            start = len(query_tensors[j]) - 1\n",
        "            end = start + len(response_tensors[j])\n",
        "            masks[j, :start] = 0\n",
        "            masks[j, end:] = 0\n",
        "            rewards[j, end - 1] += score_tensors[j]\n",
        "            rewards[j, :] *= masks[j, :]\n",
        "            values[j, :-1] *= masks[j, :]\n",
        "\n",
        "    return logp, rewards, values[:, :-1], masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy9FJ_jFcyFC",
        "outputId": "f0357fa7-0f98-483e-a57c-e98cd518aa1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        0.6719, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "       device='cuda:0', dtype=torch.bfloat16)\n",
            "tensor([    2,     2,     2,     2,     2,     2,     2,   523, 28766,  1838,\n",
            "        28766, 28767,    13,  5660,   511,  9923,   625,  3408, 28804,     2,\n",
            "        28705,    13, 28789, 28766,   489, 11143, 28766, 28767,    13,  2249,\n",
            "         1549,   625,  3408,  1059,   264,  1759,  1987,  8886, 28724,   448,\n",
            "        21537, 28723,  6213,  8886, 28724,   448, 21537, 28725,   484,  5638,\n",
            "         3126, 19530,   325, 28708,  5344, 18958,   466, 28731,   297,   272,\n",
            "         5100, 28742, 28713,  8894, 10612,  1816,  2061,  3408,   477,   272,\n",
            "         4376, 28723], device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "logprobs, rewards, values, masks = compute_rewards(input_data, query_tensors, response_tensors, score_tensors)\n",
        "print(rewards[0])\n",
        "print(input_data['input_ids'][0])\n",
        "print(input_data['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBHebohxdFR5",
        "outputId": "1b166cd7-94d5-40a6-8085-fd8a7c46e50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -0.0000e+00, -1.7422e+00, -6.3438e+00, -3.2344e+00,  3.6875e+00,\n",
            "         1.4062e+00,  5.6250e+00, -1.7109e+00,  8.5625e+00,  8.5625e+00,\n",
            "         9.5000e+00,  8.6250e+00,  9.9375e+00,  2.7812e+00,  7.3125e+00,\n",
            "         9.0625e+00,  1.0062e+01,  1.1250e+01,  1.6000e+01,  6.7812e+00,\n",
            "         9.1250e+00,  5.4375e+00,  8.8750e+00,  1.2500e+01,  1.5000e+01,\n",
            "         1.0750e+01,  1.0562e+01,  1.0500e+01,  5.9688e+00,  1.7875e+01,\n",
            "         9.6250e+00,  1.0062e+01,  5.0312e+00,  8.0625e+00,  1.0859e+00,\n",
            "         6.1250e+00,  1.7578e-01,  1.4188e+01,  3.6250e+00,  4.2188e+00,\n",
            "         7.0625e+00, -1.4893e-02,  1.6016e+00,  8.3750e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00], device='cuda:0', dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "print(masks[0])\n",
        "print(values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnlh9TEeH-h"
      },
      "source": [
        "## Compute Advantage\n",
        "\n",
        "The advantage function estimates how much better an action is compared to the average:\n",
        "- **Positive advantage**: This action is better than expected\n",
        "- **Negative advantage**: This action is worse than expected\n",
        "\n",
        "Uses Generalized Advantage Estimation (GAE) for lower variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6rL-IzH8eVZN"
      },
      "outputs": [],
      "source": [
        "def masked_mean(values, mask):\n",
        "    return (values * mask).sum() / mask.sum()\n",
        "\n",
        "def masked_var(values, mask):\n",
        "    mean = masked_mean(values, mask)\n",
        "    centred_values = values - mean\n",
        "    return masked_mean(centred_values ** 2, mask)\n",
        "\n",
        "def masked_whiten(values, mask):\n",
        "    mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
        "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
        "    whitened += mean\n",
        "    return whitened\n",
        "\n",
        "def compute_advantage(rewards, values, masks):\n",
        "    lastgae = 0.0\n",
        "    advantage_reversed = []\n",
        "    seq_length = rewards.shape[-1]\n",
        "    gamma, lam = 1.0, 0.95\n",
        "\n",
        "    for t in reversed(range(seq_length)):\n",
        "        nextvalues = values[:, t + 1] if t < seq_length - 1 else 0.0\n",
        "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
        "        lastgae = delta + gamma * lam * lastgae\n",
        "        advantage_reversed.append(lastgae)\n",
        "    advantages = torch.stack(advantage_reversed[::-1], dim=1)\n",
        "    advantages = masked_whiten(advantages, masks)\n",
        "\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ZTjweyijUu",
        "outputId": "b6ede7dc-6ed6-4fb4-eb63-92705b41eff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.5703, -0.5508, -0.5273, -0.5039, -0.4805, -0.4531, -0.4297, -0.4023,\n",
            "        -0.3711, -0.3477, -0.3047, -0.2812, -0.2422, -0.2031, -0.1641, -0.1250,\n",
            "        -0.0859, -0.0312,  0.0078,  0.0625,  0.1172,  0.5625,  1.6562,  1.1094,\n",
            "        -0.3320,  0.2109, -0.6641,  0.9688, -1.1953, -1.2109, -1.4297, -1.2656,\n",
            "        -1.5703, -0.0156, -0.9688, -1.3594, -1.6016, -1.8984, -3.0000, -1.0625,\n",
            "        -1.5938, -0.8086, -1.5625, -2.4062, -3.0312, -2.2031, -2.2344, -2.2812,\n",
            "        -1.3438, -4.0000, -2.3125, -2.5000, -1.4609, -2.1562, -0.6836, -1.7812,\n",
            "        -0.5156, -3.6094, -1.3906, -1.5469, -2.2188, -0.7148, -1.0625, -2.5781,\n",
            "        -0.9492, -0.9492, -0.9492, -0.9492, -0.9492, -0.9492, -0.9492],\n",
            "       device='cuda:0', dtype=torch.bfloat16)\n",
            "tensor([-5.7031e-01, -5.5078e-01, -5.2734e-01, -5.0391e-01, -4.8047e-01,\n",
            "        -4.5312e-01, -4.2969e-01, -4.0234e-01, -3.7109e-01, -3.4766e-01,\n",
            "        -3.0469e-01, -2.8125e-01, -2.4219e-01, -2.0312e-01, -1.6406e-01,\n",
            "        -1.2500e-01, -8.5938e-02, -3.1250e-02,  7.8125e-03,  6.2500e-02,\n",
            "         1.1719e-01, -1.1797e+00, -4.6875e+00, -2.1250e+00,  3.3594e+00,\n",
            "         1.6172e+00,  4.9688e+00, -7.4219e-01,  7.3750e+00,  7.3438e+00,\n",
            "         8.0625e+00,  7.3750e+00,  8.3750e+00,  2.7656e+00,  6.3438e+00,\n",
            "         7.6875e+00,  8.4375e+00,  9.3750e+00,  1.3000e+01,  5.7188e+00,\n",
            "         7.5312e+00,  4.6250e+00,  7.3125e+00,  1.0125e+01,  1.2000e+01,\n",
            "         8.5625e+00,  8.3125e+00,  8.2500e+00,  4.6250e+00,  1.3875e+01,\n",
            "         7.3125e+00,  7.5625e+00,  3.5625e+00,  5.9062e+00,  4.0234e-01,\n",
            "         4.3438e+00, -3.3984e-01,  1.0562e+01,  2.2344e+00,  2.6719e+00,\n",
            "         4.8438e+00, -7.3047e-01,  5.3906e-01,  5.8125e+00, -9.4922e-01,\n",
            "        -9.4922e-01, -9.4922e-01, -9.4922e-01, -9.4922e-01, -9.4922e-01,\n",
            "        -9.4922e-01], device='cuda:0', dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "advantages, returns = compute_advantage(rewards, values, masks)\n",
        "print(advantages[0])\n",
        "print(returns[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dACSgd1ripo_"
      },
      "source": [
        "## Mini-batch PPO Training\n",
        "\n",
        "PPO updates the policy using mini-batches to improve sample efficiency and stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTTWJrvLxaUz"
      },
      "source": [
        "### Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UsqMGuocxdJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04376146-1f59-45c0-b3b3-edf665a3bce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer created with lr=1e-05\n"
          ]
        }
      ],
      "source": [
        "# Training hyperparameters\n",
        "# NOTE: These may need adjustment for Zephyr 7B\n",
        "learning_rate = 1e-5  # Conservative learning rate for fine-tuning\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "print(f\"Optimizer created with lr={learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwYclizbyNg1",
        "outputId": "7635581f-b249-4cf8-a273-2fc78000e9ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "np.random.permutation(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vhiRjkGCzBLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e892b61e-a69f-4a9a-9621-92c489e89eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPO config: 4 epochs, mini_batch_size=2\n"
          ]
        }
      ],
      "source": [
        "# PPO training configuration\n",
        "mini_batch_size = 2  # Reduced from 4 for memory efficiency with 7B model\n",
        "ppo_epochs = 4\n",
        "\n",
        "cliprange_ratio = 0.2  # PPO clipping range\n",
        "v_loss_coeff = 0.1     # Value loss coefficient\n",
        "ratio_threshold = 10   # Threshold to detect unstable training\n",
        "\n",
        "def compute_loss(old_logprobs, values, logprobs, vpreds, masks, advantages, returns):\n",
        "    \"\"\"\n",
        "    Compute PPO loss with clipping.\n",
        "\n",
        "    Args:\n",
        "        old_logprobs: Log probabilities from the old policy\n",
        "        values: Value estimates from the old policy\n",
        "        logprobs: Log probabilities from the current policy\n",
        "        vpreds: Value predictions from the current policy\n",
        "        masks: Attention masks\n",
        "        advantages: Computed advantages\n",
        "        returns: Computed returns (advantages + values)\n",
        "\n",
        "    Returns:\n",
        "        loss: Combined policy and value loss\n",
        "        v_loss: Value loss component\n",
        "    \"\"\"\n",
        "    ratio = torch.exp(logprobs - old_logprobs)\n",
        "    pg_loss1 = - ratio * advantages\n",
        "    pg_loss2 = - torch.clamp(ratio, 1 - cliprange_ratio, 1 + cliprange_ratio) * advantages\n",
        "    pg_loss = masked_mean(torch.max(pg_loss1, pg_loss2), masks)\n",
        "\n",
        "    v_loss = masked_mean((vpreds - returns) ** 2, masks)\n",
        "    loss = pg_loss + v_loss_coeff * v_loss\n",
        "\n",
        "    avg_ratio = masked_mean(ratio, masks)\n",
        "    if avg_ratio > ratio_threshold:\n",
        "        # Unstable training detected - zero out gradients\n",
        "        pg_loss = pg_loss * 0.0\n",
        "        v_loss = v_loss * 0.0\n",
        "        loss = loss * 0.0\n",
        "\n",
        "    return loss, v_loss\n",
        "\n",
        "def mini_batch_train():\n",
        "    \"\"\"Run mini-batch PPO training for multiple epochs.\"\"\"\n",
        "    for ep in range(ppo_epochs):\n",
        "        batch_inds = np.random.permutation(batch_size)\n",
        "\n",
        "        for start in range(0, batch_size, mini_batch_size):\n",
        "            end = start + mini_batch_size\n",
        "            mini_batch_inds = batch_inds[start:end]\n",
        "\n",
        "            mb_model_inputs = {\n",
        "                'input_ids': input_data['input_ids'][mini_batch_inds],\n",
        "                'attention_mask': input_data['attention_mask'][mini_batch_inds]\n",
        "            }\n",
        "            mb_logits, mb_vpreds = model(**mb_model_inputs)\n",
        "            mb_logits = torch.nn.functional.log_softmax(mb_logits[:, :-1, :], dim=-1)\n",
        "            mb_logprobs = torch.gather(mb_logits, 2, mb_model_inputs['input_ids'][:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            loss, loss_v = compute_loss(\n",
        "                logprobs[mini_batch_inds],\n",
        "                values[mini_batch_inds],\n",
        "                mb_logprobs,\n",
        "                mb_vpreds[:, :-1],\n",
        "                masks[mini_batch_inds],\n",
        "                advantages[mini_batch_inds],\n",
        "                returns[mini_batch_inds]\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print('loss/total', loss.item())\n",
        "    print('mini-batch training finished')\n",
        "\n",
        "print(f\"PPO config: {ppo_epochs} epochs, mini_batch_size={mini_batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TmyCw7g9zGU",
        "outputId": "47d02a7e-65e1-46d2-b6be-d8f8c4faa169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss/total 1.53125\n",
            "loss/total 1.359375\n",
            "loss/total 1.4296875\n",
            "loss/total 1.2890625\n",
            "loss/total 1.265625\n",
            "loss/total 1.3515625\n",
            "loss/total 1.2265625\n",
            "loss/total 1.3203125\n",
            "mini-batch training finished\n"
          ]
        }
      ],
      "source": [
        "mini_batch_train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcZPLHlb91la"
      },
      "source": [
        "## Train RLHF\n",
        "\n",
        "The main training loop:\n",
        "1. Generate responses for each prompt\n",
        "2. Score responses with LLM judge\n",
        "3. Compute rewards (score - KL penalty)\n",
        "4. Compute advantages (how good was this response?)\n",
        "5. Update the policy with PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tsh0CnDR-zyN",
        "outputId": "3b918b52-d11a-4232-dcaa-43ac56b31e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss/total 0.734375\n",
            "loss/total 0.462890625\n",
            "loss/total 0.6328125\n",
            "loss/total 0.40625\n",
            "loss/total 0.609375\n",
            "loss/total 0.40234375\n",
            "loss/total 0.369140625\n",
            "loss/total 0.578125\n",
            "mini-batch training finished\n",
            "loss/total 0.373046875\n",
            "loss/total 0.326171875\n",
            "loss/total 0.20703125\n",
            "loss/total 0.33203125\n",
            "loss/total 0.33984375\n",
            "loss/total 0.263671875\n",
            "loss/total 0.298828125\n",
            "loss/total 0.158203125\n",
            "mini-batch training finished\n",
            "loss/total 0.353515625\n",
            "loss/total 0.375\n",
            "loss/total 0.36328125\n",
            "loss/total 0.13671875\n",
            "loss/total 0.29296875\n",
            "loss/total 0.296875\n",
            "loss/total 0.1064453125\n",
            "loss/total 0.28515625\n",
            "mini-batch training finished\n",
            "loss/total 0.146484375\n",
            "loss/total 0.1259765625\n",
            "loss/total 0.12890625\n",
            "loss/total 0.0625\n",
            "loss/total 0.07373046875\n",
            "loss/total 0.052001953125\n",
            "loss/total 0.01513671875\n",
            "loss/total 0.0751953125\n",
            "mini-batch training finished\n",
            "loss/total 0.671875\n",
            "loss/total 0.6796875\n",
            "loss/total 0.66015625\n",
            "loss/total 0.5859375\n",
            "loss/total 0.609375\n",
            "loss/total 0.5546875\n",
            "loss/total 0.56640625\n",
            "loss/total 0.55859375\n",
            "mini-batch training finished\n",
            "loss/total 0.8046875\n",
            "loss/total 0.6796875\n",
            "loss/total 0.734375\n",
            "loss/total 0.640625\n",
            "loss/total 0.6171875\n",
            "loss/total 0.67578125\n",
            "loss/total 0.59765625\n",
            "loss/total 0.703125\n",
            "mini-batch training finished\n",
            "loss/total 0.365234375\n",
            "loss/total 0.67578125\n",
            "loss/total 0.68359375\n",
            "loss/total 0.26171875\n",
            "loss/total 0.62109375\n",
            "loss/total 0.220703125\n",
            "loss/total 0.53125\n",
            "loss/total 0.1884765625\n",
            "mini-batch training finished\n",
            "loss/total 0.181640625\n",
            "loss/total 0.064453125\n",
            "loss/total 0.0029296875\n",
            "loss/total 0.0693359375\n",
            "loss/total -0.024169921875\n",
            "loss/total 0.045654296875\n",
            "loss/total -0.025390625\n",
            "loss/total 0.0859375\n",
            "mini-batch training finished\n",
            "loss/total 0.71484375\n",
            "loss/total 0.5234375\n",
            "loss/total 0.640625\n",
            "loss/total 0.47265625\n",
            "loss/total 0.58203125\n",
            "loss/total 0.47265625\n",
            "loss/total 0.4453125\n",
            "loss/total 0.64453125\n",
            "mini-batch training finished\n",
            "loss/total 0.0888671875\n",
            "loss/total nan\n",
            "loss/total nan\n",
            "loss/total nan\n",
            "loss/total nan\n",
            "loss/total nan\n",
            "loss/total nan\n",
            "loss/total nan\n",
            "mini-batch training finished\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-286197537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnew_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_min_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mgeneration_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_new_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             query_response = model.generate(\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-329908665.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2777\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2780\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2598\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Generate responses\n",
        "        query_tensors = batch['input_ids']\n",
        "        query_attention_masks = batch['attention_mask']\n",
        "\n",
        "        response_tensors = []\n",
        "        query_response_tensors = []\n",
        "        score_tensors = []\n",
        "\n",
        "        for i, query in enumerate(query_tensors):\n",
        "            query = query.to(device)\n",
        "            query_attention_mask = query_attention_masks[i].to(device)\n",
        "            new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "            query_response = model.generate(\n",
        "                input_ids=query.unsqueeze(0),\n",
        "                attention_mask=query_attention_mask.unsqueeze(0),\n",
        "                **generation_kwargs\n",
        "                ).squeeze(0)\n",
        "\n",
        "            response_len = len(query_response) - len(query)\n",
        "            response_tensors.append(query_response[-response_len:])\n",
        "            query_response_tensors.append(query_response)\n",
        "\n",
        "            # Use LLM judge instead of reward model\n",
        "            with torch.no_grad():\n",
        "                prompt_text = tokenizer.decode(query, skip_special_tokens=True)\n",
        "                response_text = tokenizer.decode(query_response[-response_len:], skip_special_tokens=True)\n",
        "\n",
        "                # Get score from LLM judge\n",
        "                score = get_llm_judge_score(prompt_text, response_text)\n",
        "                score = torch.tensor(score).to(device)\n",
        "\n",
        "            score_tensors.append(score)\n",
        "\n",
        "        input_data = data_collator([\n",
        "            {\n",
        "                'input_ids': ids,\n",
        "                'attention_mask': torch.ones_like(ids)\n",
        "            }\n",
        "            for ids in query_response_tensors\n",
        "        ]).to(device)\n",
        "\n",
        "        # rewards and advantages\n",
        "        logprobs, rewards, values, masks = compute_rewards(input_data, query_tensors, response_tensors, score_tensors)\n",
        "        advantages, returns = compute_advantage(rewards, values, masks)\n",
        "\n",
        "        # mini batch training\n",
        "        mini_batch_train()\n",
        "    print(f'epoch {epoch + 1} finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p91mWY7ZAMw9"
      },
      "source": [
        "## Validation\n",
        "\n",
        "Test the trained model on the validation set to see if responses improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUEfXiPmOybE"
      },
      "outputs": [],
      "source": [
        "len(tokenized_dataset_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xdzy5XlO07I"
      },
      "outputs": [],
      "source": [
        "val_gen_lengths = [0] * len(tokenized_dataset_val)\n",
        "for i in range(len(tokenized_dataset_val)):\n",
        "    val_gen_lengths[i] = random.choice(list(range(output_min_length, output_max_length)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPM6f46xPyxO"
      },
      "outputs": [],
      "source": [
        "val_gen_lengths[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y08Nx89CP2je"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    scores = []\n",
        "    for b, batch in enumerate(val_dataloader):\n",
        "        # Generate_responses\n",
        "        query_tensors = batch['input_ids']\n",
        "        query_attention_masks = batch['attention_mask']\n",
        "        for i, query in enumerate(query_tensors):\n",
        "            query = query.to(device)\n",
        "            query_attention_mask = query_attention_masks[i].to(device)\n",
        "            new_tokens = val_gen_lengths[b * len(query_tensors) + i]\n",
        "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "            query_response = model.generate(\n",
        "                input_ids=query.unsqueeze(0),\n",
        "                attention_mask=query_attention_mask.unsqueeze(0),\n",
        "                **generation_kwargs\n",
        "                ).squeeze(0)\n",
        "            query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
        "            attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
        "            score = reward_model(query_response_score.unsqueeze(0), attention_mask.unsqueeze(0)).squeeze(0)[-1]\n",
        "            score = 2 * (score - 0.5)\n",
        "            scores.append(score.item())\n",
        "    print('avg score:', sum(scores) / len(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvD7FCQ3SF7K"
      },
      "outputs": [],
      "source": [
        "validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOnXVMV7TaRy"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'ppo_model_epoch_1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwf0VWEuSG0G"
      },
      "outputs": [],
      "source": [
        "model_path = './sft_model_epoch_1'\n",
        "model = ModelForCausalLMWithValueHead(model_path).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40GywjqISiAW"
      },
      "outputs": [],
      "source": [
        "validate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad2d2ef1bab84bbb994081448f5e24bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d116cf3fc523425b85d777bed99ca654",
              "IPY_MODEL_8f1a06c2b144420aa844b1afd9562b36",
              "IPY_MODEL_d3343f64aad84751bd9d44fbc6ff2b35"
            ],
            "layout": "IPY_MODEL_c13404bfe14344d781a863559a6b4b3c"
          }
        },
        "d116cf3fc523425b85d777bed99ca654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0de6966bd48e490da467c7314f62c48b",
            "placeholder": "​",
            "style": "IPY_MODEL_28266d13f1da49a7b14e8dd28ad4438f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8f1a06c2b144420aa844b1afd9562b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0bb0909eab424d840c18848e684099",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3a41b26942f4d7a9b060b313f2eb31f",
            "value": 8
          }
        },
        "d3343f64aad84751bd9d44fbc6ff2b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_703ef3fd72a043529769fad9351d1328",
            "placeholder": "​",
            "style": "IPY_MODEL_112c1793e9e8471fbd58d2b9ff888a8d",
            "value": " 8/8 [00:16&lt;00:00,  1.73s/it]"
          }
        },
        "c13404bfe14344d781a863559a6b4b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de6966bd48e490da467c7314f62c48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28266d13f1da49a7b14e8dd28ad4438f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d0bb0909eab424d840c18848e684099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3a41b26942f4d7a9b060b313f2eb31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "703ef3fd72a043529769fad9351d1328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112c1793e9e8471fbd58d2b9ff888a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eda750751f249dcbe38d0e19512e161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b43cd6d80fc427ba5109cf1a5f2f8cd",
              "IPY_MODEL_1b5e6ff2c510437e97aff10741dfd9e2",
              "IPY_MODEL_dc02a64433b14be49d5911efc1e0348b"
            ],
            "layout": "IPY_MODEL_8ace145730144e509135f830df765a38"
          }
        },
        "3b43cd6d80fc427ba5109cf1a5f2f8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0625677c164439bafb5b0b30f6e2b9",
            "placeholder": "​",
            "style": "IPY_MODEL_2c826ad713ec4b5ba0fe8452910aa1e1",
            "value": "Map: 100%"
          }
        },
        "1b5e6ff2c510437e97aff10741dfd9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd0a4885d80c4b499b07e2505d699a4f",
            "max": 276,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ef23ae50969464e89281e4c258dafe8",
            "value": 276
          }
        },
        "dc02a64433b14be49d5911efc1e0348b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2df22dd37cba40abb3bc418107567c7a",
            "placeholder": "​",
            "style": "IPY_MODEL_30a431d060f949dca7e96f2a597497a7",
            "value": " 276/276 [00:00&lt;00:00, 2433.83 examples/s]"
          }
        },
        "8ace145730144e509135f830df765a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0625677c164439bafb5b0b30f6e2b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c826ad713ec4b5ba0fe8452910aa1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd0a4885d80c4b499b07e2505d699a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ef23ae50969464e89281e4c258dafe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2df22dd37cba40abb3bc418107567c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a431d060f949dca7e96f2a597497a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfc2bdf9a8d74e49b4688742473cbc8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24322f82677c4fbd831d486e94a24637",
              "IPY_MODEL_aca6ce3f772e40139e4e4466c0b21c85",
              "IPY_MODEL_cb50225678a64c6fa1338a52b77e6582"
            ],
            "layout": "IPY_MODEL_c3dcf3f5c9b44751840d5b87c4b86f22"
          }
        },
        "24322f82677c4fbd831d486e94a24637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fff1719f0b4873aebca1f2010700e0",
            "placeholder": "​",
            "style": "IPY_MODEL_eca8a5b096d14c468e91610a78fbe9f3",
            "value": "Map: 100%"
          }
        },
        "aca6ce3f772e40139e4e4466c0b21c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19278d72fc3475e98bd7ce8c1d64afb",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f75118989ae42cb9dcb9b82bdf023fd",
            "value": 31
          }
        },
        "cb50225678a64c6fa1338a52b77e6582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66ec28a2119440149fafae0f1675bc54",
            "placeholder": "​",
            "style": "IPY_MODEL_88004abd7de34ed6aa02b79c75f46ce7",
            "value": " 31/31 [00:00&lt;00:00, 890.88 examples/s]"
          }
        },
        "c3dcf3f5c9b44751840d5b87c4b86f22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78fff1719f0b4873aebca1f2010700e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca8a5b096d14c468e91610a78fbe9f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c19278d72fc3475e98bd7ce8c1d64afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f75118989ae42cb9dcb9b82bdf023fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66ec28a2119440149fafae0f1675bc54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88004abd7de34ed6aa02b79c75f46ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}