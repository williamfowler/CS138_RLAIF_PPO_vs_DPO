{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kQRguV413i9"
      },
      "source": [
        "# RLAIF Fine-Tuning\n",
        "\n",
        "## Mount Google Drive\n",
        "\n",
        "Do this to be able to access a dataset you have in your Google Drive account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_2QMYbLqVIV",
        "outputId": "c01981c0-5e0e-419c-9d15-882c4f3612f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o1kH4xVflCe_",
        "outputId": "e4f61c28-ab15-4cea-955e-3f64ce697b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Need the updated version of this package\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKIA80F2NK_"
      },
      "source": [
        "## LLM Judge (Reward Function)\n",
        "\n",
        "Here is where we define the LLM judge/our fake reward model for Direct-RLAIF\n",
        "Instead of using a trained reward model, we'll use the Google Colab AI API ot prompt Gemini for a rating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g0VIlFvUgLA",
        "outputId": "e8deb1ac-3a2d-4daa-9d6c-b5edcf497948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test LLM Judge Score [GOOD]: 1.0000\n",
            "✓ LLM judge function ready (using placeholder - replace with real API later)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import time\n",
        "from google.colab import ai\n",
        "\n",
        "# --- 1. Helper Function to Call Your API ---\n",
        "def call_llm_api(prompt_content):\n",
        "    \"\"\"\n",
        "    Simulates or performs the actual API call.\n",
        "    Replace the body of this function with your actual API request (e.g., OpenAI, Anthropic).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return ai.generate_text(prompt_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Judge Call failed: {e}\")\n",
        "        return \"Score: 5\" # Fail-safe neutral score\n",
        "\n",
        "# --- 2. The Main Scoring Function ---\n",
        "def get_llm_judge_score(prompt, response):\n",
        "    \"\"\"\n",
        "    Get a reward score from an LLM judge for a given prompt-response pair.\n",
        "    Uses a 1-10 absolute rating scale, normalized to [-1, 1].\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The original prompt/question\n",
        "        response (str): The model's generated response\n",
        "\n",
        "    Returns:\n",
        "        float: Reward score in range [-1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    # A. Construct the Evaluation Prompt\n",
        "    # We ask for \"Reasoning\" first to encourage Chain-of-Thought, which improves scoring accuracy.\n",
        "    eval_prompt = f\"\"\"\n",
        "You are an expert AI evaluator. Review the following interaction:\n",
        "\n",
        "[User Instruction]\n",
        "{prompt}\n",
        "\n",
        "[AI Response]\n",
        "{response}\n",
        "\n",
        "Your task is to evaluate the helpfullness, honestly, and overall quality of the\n",
        "AI response on a scale of 1 to 10.\n",
        "- **1-3 (Poor):** Factually incorrect, harmful, or irrelevant.\n",
        "- **4-6 (Average):** Relevant but vague, incomplete, slightly hallucinated, or\n",
        "way too verbose for what the question required\n",
        "- **7-9 (Good):** Accurate, helpful, and concise.\n",
        "- **10 (Excellent):** Perfect, insightful, and well-structured.\n",
        "\n",
        "**Evaluation Protocol:**\n",
        "1. Briefly verify the factual accuracy.\n",
        "2. Check if all constraints in the instruction were met.\n",
        "3. Assign a final integer score.\n",
        "\n",
        "**Be Aware of Your Own Limitations**\n",
        "Beware of the central tendency bias in LLMs to just assign everything a good\n",
        "score. YOU SHOULD BE ASSIGNING SOME BAD SCORES AND SOME PERFECT ONES.\n",
        "\n",
        "**Output Format:**\n",
        "Reasoning: [Your reasoning here]\n",
        "Score: [Integer 1-10]\n",
        "\"\"\"\n",
        "\n",
        "    # B. Get the LLM's opinion\n",
        "    llm_output = call_llm_api(eval_prompt)\n",
        "\n",
        "    # C. Extract the Score using Regex\n",
        "    # This looks for \"Score: 7\" or just \"7\" at the end of a line\n",
        "    match = re.search(r'Score:\\s*(\\d+)', llm_output, re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        raw_score = int(match.group(1))\n",
        "    else:\n",
        "        # Fallback if parsing fails\n",
        "        print(f\"Warning: Could not parse score from LLM output. Defaulting to 5.\\nOutput: {llm_output[:100]}...\")\n",
        "        raw_score = 5\n",
        "\n",
        "    # Clamp score to 1-10 just in case\n",
        "    raw_score = max(1, min(10, raw_score))\n",
        "\n",
        "    # D. Normalize to [-1, 1] range for PPO\n",
        "    # Formula: (score - 1) / 9 gives [0, 1]. Then multiply by 2 and subtract 1.\n",
        "    # 1 -> -1.0\n",
        "    # 5 -> -0.11\n",
        "    # 10 -> 1.0\n",
        "    normalized_score = 2 * ((raw_score - 1) / 9) - 1\n",
        "\n",
        "    return float(normalized_score)\n",
        "\n",
        "# Test the placeholder function\n",
        "test_prompt = \"What is the capital of France?\"\n",
        "test_response = \"Paris is the capital and most populous city of France.\"\n",
        "test_score = get_llm_judge_score(test_prompt, test_response)\n",
        "print(f\"Test LLM Judge Score [GOOD]: {test_score:.4f}\")\n",
        "print(\"✓ LLM judge function ready (using placeholder - replace with real API later)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juFUbSNfVn2U"
      },
      "source": [
        "## Policy Model with Value Head\n",
        "\n",
        "PPO requires two components:\n",
        "- **Policy (Actor)**: The language model that generates text\n",
        "- **Value Head (Critic)**: Estimates the expected reward for a given state\n",
        "\n",
        "We combine both into a single model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQo4pbqGVv3A",
        "outputId": "3ab27c20-f91b-4225-b261-babfcf0f47bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\") #make sure this says \"cuda\"\n",
        "\n",
        "from typing import Optional\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "class ValueHead(nn.Module):\n",
        "    \"\"\"\n",
        "    The ValueHead class implements a head for the model\n",
        "    that returns a scalar for each output token.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.value = nn.Linear(self.hidden_size, 1)\n",
        "        self._post_init()\n",
        "\n",
        "    def _post_init(self):\n",
        "        nn.init.normal_(self.value.weight, std=(1.0 / np.sqrt(self.hidden_size + 1)))\n",
        "        nn.init.zeros_(self.value.bias)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        output = hidden_states\n",
        "        return self.value(output)\n",
        "\n",
        "\n",
        "class ModelForCausalLMWithValueHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Causal LM model with a value head on top.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name_or_path, quantization_config=None):\n",
        "        super().__init__()\n",
        "        # NEW: Support loading from HuggingFace with quantization\n",
        "        if quantization_config is not None:\n",
        "            self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16,\n",
        "            )\n",
        "        else:\n",
        "            # OLD: Load from local path (GPT-2)\n",
        "            self.llm = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "\n",
        "        # Add the value head\n",
        "        self.v_head = ValueHead(self.llm.config)\n",
        "\n",
        "        # IMPORTANT: Move value head to same device as the LLM\n",
        "        # With device_map=\"auto\", the LLM is on GPU but v_head might be on CPU\n",
        "        if quantization_config is not None:\n",
        "            # Find which device the LLM is on\n",
        "            try:\n",
        "                llm_device = next(self.llm.parameters()).device\n",
        "                self.v_head = self.v_head.to(llm_device)\n",
        "                print(f\"Value head moved to {llm_device}\")\n",
        "            except StopIteration:\n",
        "                print(\"Warning: Could not determine LLM device\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "    ) -> Optional[torch.FloatTensor]:\n",
        "\n",
        "        transformer_outputs = self.llm.forward(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states = True,\n",
        "        )\n",
        "        lm_logits = transformer_outputs.logits\n",
        "        # Get the last hidden state\n",
        "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
        "\n",
        "        # Apply the value head\n",
        "        value = self.v_head(last_hidden_state).squeeze(-1)\n",
        "        return lm_logits, value\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.llm.generate(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSlYRp9qfIwC"
      },
      "source": [
        "## Load the model from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "882ce8572b1841bcbe1dd2cc97bdcf7b",
            "3ef1c5ad46304c9a8a2efaef0ab3d3ba",
            "9d3924e9571d4a11917509f1b5baffc3",
            "7d2f7bfe2a36424691182d03cc90bdcb",
            "662f7f2b3c284365af7e38e415892a7b",
            "098155374c7f43fcbf13101484501f60",
            "6ae6d60c69b34f8c846ce2643285ceb4",
            "bd8f2918a48c40caacd2c3cb1cea637b",
            "e19b89980a924b3b8a17c63917268522",
            "46bd263ea7664b3db23d5e0439b3339a",
            "4b0031ba1cbb429a9a2d042fe8848d20"
          ]
        },
        "id": "X3xGmHI8W0Q0",
        "outputId": "14e64f05-29cb-4ddd-cb54-2c9ec459ad16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "882ce8572b1841bcbe1dd2cc97bdcf7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value head moved to cuda:0\n",
            "Zephyr 7B model loaded successfully with value head!\n"
          ]
        }
      ],
      "source": [
        "# Load Zephyr 7B from HuggingFace with 4-bit quantization\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Quantization config for 4-bit loading\n",
        "# note: might have to change bfloat16 to float16 depending on GPU\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model with value head\n",
        "model = ModelForCausalLMWithValueHead(model_name, quantization_config=bnb_config)\n",
        "print(\"Zephyr 7B model loaded successfully with value head!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqUTbr_DXA0j"
      },
      "source": [
        "## Preparing Dataset\n",
        "\n",
        "We load prompts from our CSV file and tokenize them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpFxrOLgXTkz",
        "outputId": "66e2ba10-0e32-4fe0-e854-e7f3464425b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded. Pad token: </s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load Zephyr tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Important for batch generation\n",
        "print(f\"Tokenizer loaded. Pad token: {tokenizer.pad_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y32xsqLokOKi",
        "outputId": "8acf0ae4-e0c9-4b89-face-2eef7c497db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: 2000 prompts\n",
            "Columns: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected']\n",
            "\n",
            "Sample prompt: A 6 month stakeholder engagement plan for climate change-related flooding across NYC. Engaging social groups that are most vulnerable to impacts. Engaging local experts who have insights and knowledge. Weekly schedule of meetings, workshops, surveys, etc.\n"
          ]
        }
      ],
      "source": [
        "# Updated Data Loading Code\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# 1. Load the specific split designed for PPO/Generation\n",
        "# The 'train_gen' split contains prompts suitable for generation tasks\n",
        "full_dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_gen\")\n",
        "\n",
        "# 2. Select a subset of the data\n",
        "# RECOMMENDATION: Start with 2,000 for a quick run, or 10,000+ for better results.\n",
        "# We shuffle to get a random assortment of prompts.\n",
        "num_samples = 2000\n",
        "dataset_subset = full_dataset.shuffle(seed=2981).select(range(num_samples))\n",
        "\n",
        "print(f\"Dataset loaded: {len(dataset_subset)} prompts\")\n",
        "print(f\"Columns: {dataset_subset.column_names}\")\n",
        "print(f\"\\nSample prompt: {dataset_subset[0]['prompt']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8r6wI0ZXpqS",
        "outputId": "d7ccc5f9-6df0-430e-ff54-4f646571d0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 1800\n",
            "Val size: 200\n"
          ]
        }
      ],
      "source": [
        "# Create train/val split from our dataset\n",
        "# might be able to make this 100% train\n",
        "train_size = int(0.9 * len(dataset_subset))\n",
        "ds_train = dataset_subset.select(range(train_size))\n",
        "ds_val = dataset_subset.select(range(train_size, len(dataset_subset)))\n",
        "\n",
        "print(f\"Train size: {len(ds_train)}\")\n",
        "print(f\"Val size: {len(ds_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDWiRWCf25Y"
      },
      "source": [
        "# Define the Tokenizer\n",
        "One important thing to note is that this is adding in the formatted prompt \"<|user|>\\nsample<....\" so it is expecting the prompt dataset to not have this already. This should print out a sample prompt in this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB0Msyo7Y6B5",
        "outputId": "9d2a3bdc-4a3d-4b2d-c8b9-fec0540ed6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized 1800 training prompts\n",
            "Tokenized 200 validation prompts\n",
            "\n",
            "Sample formatted prompt:\n",
            "'<|user|>\\nA 6 month stakeholder engagement plan for climate change-related flooding across NYC. Engaging social groups that are most vulnerable to impacts. Engaging local experts who have insights and knowledge. Weekly schedule of meetings, workshops, surveys, etc.</s>\\n<|assistant|>\\n'\n",
            "\n",
            "Token IDs (first 20): [523, 28766, 1838, 28766, 28767, 13, 28741, 28705, 28784, 2102, 15790, 8229, 15613, 2623, 354, 11259, 2268, 28733, 9646, 2175]\n",
            "Number of tokens: 68\n"
          ]
        }
      ],
      "source": [
        "# Tokenize prompts using Zephyr's chat template\n",
        "def tokenize(sample):\n",
        "    # Format the prompt using Zephyr's chat template\n",
        "    # Zephyr expects: <|user|>\\n{prompt}</s>\\n<|assistant|>\\n\n",
        "\n",
        "    try:\n",
        "        # Try using the built-in chat template\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": sample['prompt']}\n",
        "        ]\n",
        "\n",
        "        # Use tokenize=True to get token IDs directly\n",
        "        encoded = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=None  # Return list, not tensor\n",
        "        )\n",
        "\n",
        "        # Get the formatted text for debugging\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Fallback: Manually format using Zephyr's template\n",
        "        # Based on: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
        "        formatted_prompt = f\"<|user|>\\n{sample['prompt']}</s>\\n<|assistant|>\\n\"\n",
        "        # Use add_special_tokens=True to properly handle special tokens\n",
        "        encoded = tokenizer.encode(formatted_prompt, add_special_tokens=True)\n",
        "\n",
        "    sample['input_ids'] = encoded\n",
        "    sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
        "    sample['query'] = formatted_prompt  # Keep formatted prompt text\n",
        "    return sample\n",
        "\n",
        "map_kwargs = {\n",
        "    \"batched\": False,\n",
        "    \"remove_columns\": ['prompt', 'chosen', 'rejected']  # Remove CSV columns, keep tokenized data\n",
        "}\n",
        "\n",
        "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
        "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_dataset_train)} training prompts\")\n",
        "print(f\"Tokenized {len(tokenized_dataset_val)} validation prompts\")\n",
        "print(f\"\\nSample formatted prompt:\")\n",
        "print(repr(tokenized_dataset_train[0]['query']))\n",
        "print(f\"\\nToken IDs (first 20): {tokenized_dataset_train[0]['input_ids'][:20]}\")\n",
        "print(f\"Number of tokens: {len(tokenized_dataset_train[0]['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmragNXwZxEz"
      },
      "outputs": [],
      "source": [
        "# IDK what this does, but it's probably needed\n",
        "tokenized_dataset_train.set_format(type='torch')\n",
        "tokenized_dataset_val.set_format(type='torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "e236bfcf324a4aabbacfcaba2972166c",
            "7afdf763b19749f7a9fcae9d46194c60",
            "41b8f0c1f75045e7b829e1be2847a9e7",
            "b2f4629493784dc7b1f60042c205b967",
            "10c5675f65f24ab3b7a7dc2bab6bd346",
            "fc325f7c485d4c8cba4aadc4f508cff2",
            "44a76570ee3348a5acfb4c9ec54e35af",
            "1a562dbb95c84e6bad1a6fbdb166a553",
            "26a59e6f68884969904d6d75a1f03f1b",
            "534eb88651444ca0927356c6555c2c2a",
            "6d30abd474164a43863deed3dbef754e",
            "07219d82508149a68e92db965b3f54fd",
            "7506c22b41bd4f34b270d04a0733c9c7",
            "ce05388fd7184f04a46443845342588d",
            "58e7377bfbdc44e2bd702ec9377eed41",
            "393e7ec9512744698c9e3f069d152419",
            "b00a8f3ee230498294d2a3420e15b651",
            "03e4b5e63f65430a9f8b40036712d9e6",
            "6793babb319c4164b24a565ac7f9e6a8",
            "59cc4a99088c4e5fa73f0c5b9988c245",
            "360ec0546e20467d824dbe25f9309de4",
            "f36b916f761d48c3809a78ebe17abd1f"
          ]
        },
        "id": "axTqKgJxI6Cb",
        "outputId": "1baf4201-98c7-4ad9-f5c1-3aba14afb310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Training Set Size: 1800\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e236bfcf324a4aabbacfcaba2972166c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Training Set Size: 1618\n",
            "Original Val Set Size: 200\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07219d82508149a68e92db965b3f54fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Val Set Size: 174\n"
          ]
        }
      ],
      "source": [
        "# Insert this after your 'tokenize' function and before creating DataLoaders\n",
        "\n",
        "# 1. Define your GPU's limit\n",
        "# For a T4 GPU (Colab free), 512-768 is a safe limit.\n",
        "# For A100, you can go to 2048.\n",
        "MAX_PROMPT_LENGTH = 512\n",
        "\n",
        "def filter_long_prompts(sample):\n",
        "    # Keep only samples where the prompt is short enough\n",
        "    return len(sample['input_ids']) <= MAX_PROMPT_LENGTH\n",
        "\n",
        "# 2. Apply filtering\n",
        "print(f\"Original Training Set Size: {len(tokenized_dataset_train)}\")\n",
        "tokenized_dataset_train = tokenized_dataset_train.filter(filter_long_prompts)\n",
        "print(f\"Filtered Training Set Size: {len(tokenized_dataset_train)}\")\n",
        "\n",
        "print(f\"Original Val Set Size: {len(tokenized_dataset_val)}\")\n",
        "tokenized_dataset_val = tokenized_dataset_val.filter(filter_long_prompts)\n",
        "print(f\"Filtered Val Set Size: {len(tokenized_dataset_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxG13E1haCh7"
      },
      "source": [
        "## Reward Token\n",
        "\n",
        "The reward token marks where we compute the final reward score for a generated sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqjWwhamIGH-",
        "outputId": "a4e0b95a-5d5c-4dac-982f-1fc1980482f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataloaders created with batch_size=2\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# NOTE: Batch size may need to be reduced for Zephyr 7B (larger than GPT-2)\n",
        "# Start with small batch size and increase if memory allows\n",
        "# note we're using batch_size = 8 for training. Not sure if this needs to match\n",
        "batch_size = 2  # Reduced from 32 for 7B model\n",
        "\n",
        "def collator(batch):\n",
        "    return dict((key, [d[key] for d in batch]) for key in batch[0])\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset_train, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
        "val_dataloader = DataLoader(tokenized_dataset_val, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
        "\n",
        "print(f\"Dataloaders created with batch_size={batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uCy92clTKgl7",
        "outputId": "361f0250-c818-4a08-e2ec-5048fe8ddaa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt_id': ['2b223ac0550faaca92222a2131bf3d66ac313e4c95068f0d8ad9eddd4389ea87',\n",
              "  '49d5f2cdb3f6751d392fa79b8b4de2d83d23f0d7325dc1678bcfda90d8114a51'],\n",
              " 'messages': [[{'content': 'In this task, you are given two sentences. Your task is to classify the given sentences as \"Yes\" if they have same meaning; otherwise, classify them as \"No\". \\n\\nExample Input: Sentence-1: I\\'ve never gotten into them.<sep>Sentence-2: Like readers digest .\\nExample Output: No\\n\\nExample Input: Sentence-1: I pick up shifts when I want to.<sep>Sentence-2: I work at the weekends .\\nExample Output: Yes\\n\\nExample Input: Sentence-1: We are still waiting.<sep>Sentence-2: I did get my new office .\\nExample Output:',\n",
              "    'role': 'user'}],\n",
              "  [{'content': 'Write a Python program that receives a text file as input and analyzes it, extracting the most relevant keywords from the text. The program should prioritize the most common and meaningful words, such as nouns and verbs, and ignore stop words like \"the\" or \"and\". The extracted keywords should be sorted by frequency and displayed to the user. Additionally, the program should offer an optional parameter to include or exclude specific words or phrases from the analysis.',\n",
              "    'role': 'user'}]],\n",
              " 'score_chosen': [tensor(8.), tensor(7.)],\n",
              " 'score_rejected': [tensor(8.), tensor(6.)],\n",
              " 'input_ids': [tensor([  523, 28766,  1838, 28766, 28767,    13,   657,   456,  3638, 28725,\n",
              "            368,   460,  2078,   989, 23748, 28723,  3604,  3638,   349,   298,\n",
              "            875,  1575,   272,  2078, 23748,   390,   345,  5613, 28739,   513,\n",
              "            590,   506,  1348,  5746, 28745,  5860, 28725,   875,  1575,   706,\n",
              "            390,   345,  2501,  2586, 28705,    13,    13, 20275, 11232, 28747,\n",
              "            318,   308,   636, 28733, 28740, 28747,   315, 28742,   333,  1484,\n",
              "          10930,   778,   706, 26364, 21571, 28767, 26968,   636, 28733, 28750,\n",
              "          28747,  5410, 12076, 18922,   842,    13, 20275, 15985, 28747,  1770,\n",
              "             13,    13, 20275, 11232, 28747,   318,   308,   636, 28733, 28740,\n",
              "          28747,   315,  3088,   582, 23573,   739,   315,   947,   298, 26364,\n",
              "          21571, 28767, 26968,   636, 28733, 28750, 28747,   315,   771,   438,\n",
              "            272,  1819,  2827,   842,    13, 20275, 15985, 28747,  5592,    13,\n",
              "             13, 20275, 11232, 28747,   318,   308,   636, 28733, 28740, 28747,\n",
              "            816,   460,  1309,  5345, 26364, 21571, 28767, 26968,   636, 28733,\n",
              "          28750, 28747,   315,   863,   625,   586,   633,  4007,   842,    13,\n",
              "          20275, 15985, 28747,     2, 28705,    13, 28789, 28766,   489, 11143,\n",
              "          28766, 28767,    13]),\n",
              "  tensor([  523, 28766,  1838, 28766, 28767,    13,  5238,   264, 21366,  2007,\n",
              "            369, 21415,   264,  2245,  1729,   390,  2787,   304, 10148, 12189,\n",
              "            378, 28725,  9131,   288,   272,  1080,  8598, 28049,   477,   272,\n",
              "           2245, 28723,   415,  2007,  1023,  4681, 23175,   272,  1080,  3298,\n",
              "            304, 19258,  3085, 28725,  1259,   390,   307,  1068, 28713,   304,\n",
              "           1429,  1816, 28725,   304,  9303,  2115,  3085,   737,   345,  1237,\n",
              "          28739,   442,   345,   391,  2586,   415, 25081, 28049,  1023,   347,\n",
              "          17952,   486, 11010,   304, 13992,   298,   272,  2188, 28723, 16569,\n",
              "          28725,   272,  2007,  1023,  2405,   396, 11284,  5621,   298,  3024,\n",
              "            442, 25482,  2948,  3085,   442, 27512,   477,   272,  5643, 28723,\n",
              "              2, 28705,    13, 28789, 28766,   489, 11143, 28766, 28767,    13])],\n",
              " 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n",
              " 'query': ['<|user|>\\nIn this task, you are given two sentences. Your task is to classify the given sentences as \"Yes\" if they have same meaning; otherwise, classify them as \"No\". \\n\\nExample Input: Sentence-1: I\\'ve never gotten into them.<sep>Sentence-2: Like readers digest .\\nExample Output: No\\n\\nExample Input: Sentence-1: I pick up shifts when I want to.<sep>Sentence-2: I work at the weekends .\\nExample Output: Yes\\n\\nExample Input: Sentence-1: We are still waiting.<sep>Sentence-2: I did get my new office .\\nExample Output:</s>\\n<|assistant|>\\n',\n",
              "  '<|user|>\\nWrite a Python program that receives a text file as input and analyzes it, extracting the most relevant keywords from the text. The program should prioritize the most common and meaningful words, such as nouns and verbs, and ignore stop words like \"the\" or \"and\". The extracted keywords should be sorted by frequency and displayed to the user. Additionally, the program should offer an optional parameter to include or exclude specific words or phrases from the analysis.</s>\\n<|assistant|>\\n']}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W23GG8TLKv43",
        "outputId": "5d6e9057-9c5d-4856-81d3-f62a3106c92e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation settings: 20-100 tokens\n"
          ]
        }
      ],
      "source": [
        "# Generation settings for model responses\n",
        "# NOTE: These values control response length - adjust based on your needs\n",
        "output_min_length = 20  # Increased from 5 for Zephyr\n",
        "output_max_length = 100  # Increased from 16 for more complete responses\n",
        "\n",
        "# https://huggingface.co/docs/trl/how_to_train#how-to-generate-text-for-training\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 0, # this was changed from -1\n",
        "    \"top_k\": 50, # this was changed from 0.0\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id\n",
        "}\n",
        "\n",
        "print(f\"Generation settings: {output_min_length}-{output_max_length} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcIGNP_Zh05i"
      },
      "source": [
        "## Sample Generation (Test)\n",
        "\n",
        "Let's test that our model can generate responses before starting training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJrexC5oh2W8",
        "outputId": "e4ed2ec7-ef15-45e9-c9d3-0c86597daaae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [1, 15359, 28725, 456], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "sample = tokenizer('Hi, this')\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afkv6jkjiJgp",
        "outputId": "2dc5e30b-1b2b-4667-adc6-6c3158fc76a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([    1, 15359, 28725,   456,   349,   475, 28723,  8181,   304, 10058,\n",
              "          298,  1698,  6073,  1704, 28723,   315,  3317,   368, 28742,   267,\n",
              "          544,  2548,  1162,   304,  7484,   456,  3518, 10865, 28745,   297,\n",
              "         3154, 28742, 28713,  1704, 28725,   478, 28742,   267,  1404,   298,\n",
              "         2796,  1581,  4514,   302,  4400,  2621, 17869,   369,   460,  2492,\n",
              "         8154, 14650,   297,   272,  7153, 13894, 28723,    13,    13,  2565,\n",
              "         1395,   693,   460,   633,   298,   456,  3518, 28725,   478, 28742,\n",
              "          267,  9045,   356,  7501, 12076,   395, 12302, 20715,  5202,   298,\n",
              "         3270,  7080], device='cuda:0')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_response = model.generate(\n",
        "    input_ids=torch.tensor(sample['input_ids']).unsqueeze(0).to(device),\n",
        "    attention_mask=torch.tensor(sample['attention_mask']).unsqueeze(0).to(device),\n",
        "    **generation_kwargs\n",
        "    ).squeeze(0)\n",
        "query_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "eh4EEAF6ioeQ",
        "outputId": "f5a2b8fd-e2c4-44e3-8c87-21bb1535d6d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<s> Hi, this is J. Lee and welcome to another blog post. I hope you're all doing well and finding this series helpful; in today's post, we're going to cover different types of website design trends that are making splashes in the digital landscape.\\n\\nFor those who are new to this series, we're focused on providing readers with valuable insights related to online marketing\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(query_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Np4NQ_iuAX",
        "outputId": "a88d8679-6b15-42e3-f7ef-0ca4e8fc9983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Hi, this\n",
            "Response: Hi, this is J. Lee and welcome to another blog post. I hope you're all doing well and finding this series helpful; in today's post, we're going to cover different types of website design trends that are making splashes in the digital landscape.\n",
            "\n",
            "For those who are new to this series, we're focused on providing readers with valuable insights related to online marketing\n",
            "LLM Judge Score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Test scoring a generated response with LLM judge\n",
        "with torch.no_grad():\n",
        "    # Decode the generated response\n",
        "    response_text = tokenizer.decode(query_response, skip_special_tokens=True)\n",
        "    prompt_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "    # Get score from LLM judge (replaces reward model)\n",
        "    score = get_llm_judge_score(prompt_text, response_text)\n",
        "\n",
        "    print(f\"Prompt: {prompt_text}\")\n",
        "    print(f\"Response: {response_text}\")\n",
        "    print(f\"LLM Judge Score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLxKzwl8jxQ_"
      },
      "source": [
        "## Batch Generation\n",
        "\n",
        "Now let's test generating responses for a full batch of prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMyH9qhQj8Fg",
        "outputId": "0d492457-6354-4236-8544-0926487d8e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Generating responses for 2 prompts...\n",
            "<|user|>\n",
            "In this task, you are given two sentences. Your task is to classify the given sentences as \"Yes\" if they have same meaning; otherwise, classify them as \"No\". \n",
            "\n",
            "Example Input: Sentence-1: I've never gotten into them.<sep>Sentence-2: Like readers digest .\n",
            "Example Output: No\n",
            "\n",
            "Example Input: Sentence-1: I pick up shifts when I want to.<sep>Sentence-2: I work at the weekends .\n",
            "Example Output: Yes\n",
            "\n",
            "Example Input: Sentence-1: We are still waiting.<sep>Sentence-2: I did get my new office .\n",
            "Example Output: \n",
            "<|assistant|>\n",
            "\n",
            "<|user|>\n",
            "Write a Python program that receives a text file as input and analyzes it, extracting the most relevant keywords from the text. The program should prioritize the most common and meaningful words, such as nouns and verbs, and ignore stop words like \"the\" or \"and\". The extracted keywords should be sorted by frequency and displayed to the user. Additionally, the program should offer an optional parameter to include or exclude specific words or phrases from the analysis. \n",
            "<|assistant|>\n",
            "\n",
            "Generated responses:\n",
            "['No\\n\\nExample Input: Sentence-1: Our company offers training programs.<sep>Sentence-2: Our staff receives regular training .\\nExample Output: Yes\\n\\nExample Input: Sentence-1: My car usually breaks down.<sep>Sentence-2: I frequently have car troubles .\\nExample Output: Yes\\n\\nExample Input: Sentence-1: I love reading novels before they are released.<sep', \"To create such a program, you can utilize the Natural Language Toolkit (nltk) library in Python. Here's an example implementation:\\n\\n```python\\nimport nltk\\nfrom nltk.corpus\"]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# NOTE: Model already on device via device_map=\"auto\", so we don't move it\n",
        "# model = model.to(device)  # Not needed with device_map=\"auto\"\n",
        "\n",
        "query_tensors = batch['input_ids']\n",
        "query_attention_masks = batch['attention_mask']\n",
        "\n",
        "response_tensors = []\n",
        "query_response_tensors = []\n",
        "score_tensors = []\n",
        "\n",
        "print(f\"Generating responses for {len(query_tensors)} prompts...\")\n",
        "\n",
        "for i, query in enumerate(query_tensors):\n",
        "    query = query.to(device)\n",
        "    query_attention_mask = query_attention_masks[i].to(device)\n",
        "    new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "    generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "    query_response = model.generate(\n",
        "        input_ids=query.unsqueeze(0),\n",
        "        attention_mask=query_attention_mask.unsqueeze(0),\n",
        "        **generation_kwargs\n",
        "    ).squeeze(0)\n",
        "\n",
        "    response_len = len(query_response) - len(query)\n",
        "    response_tensors.append(query_response[-response_len:])\n",
        "    query_response_tensors.append(query_response)\n",
        "\n",
        "    # Use LLM judge instead of reward model\n",
        "    with torch.no_grad():\n",
        "        prompt_text = tokenizer.decode(query, skip_special_tokens=True)\n",
        "        print(prompt_text)\n",
        "        response_text = tokenizer.decode(query_response[-response_len:], skip_special_tokens=True)\n",
        "\n",
        "        # Get score from LLM judge\n",
        "        score = get_llm_judge_score(prompt_text, response_text)\n",
        "        score = torch.tensor(score).to(device)\n",
        "\n",
        "    score_tensors.append(score)\n",
        "\n",
        "batch[\"response\"] = [tokenizer.decode(response, skip_special_tokens=True) for response in response_tensors]\n",
        "print(\"Generated responses:\")\n",
        "print(batch['response'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1K4nAUG5fes"
      },
      "source": [
        "## Compute Reward\n",
        "\n",
        "The reward function combines:\n",
        "- **Score from LLM judge**: Quality of the response\n",
        "- **KL penalty**: Prevents the model from diverging too much from the reference (SFT) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-zd0KK85pGT"
      },
      "source": [
        "**Reward Formula:**\n",
        "\n",
        "$\\text{reward} = \\text{score} - \\beta \\cdot \\log \\left(\\frac{\\pi^{RL}_\\theta}{\\pi^{SFT}}\\right)$\n",
        "\n",
        "Where:\n",
        "- $\\text{score}$ is from the LLM judge\n",
        "- $\\beta$ is the KL penalty coefficient (controls how much we penalize divergence)\n",
        "- $\\pi^{RL}_\\theta$ is the current policy (model being trained)\n",
        "- $\\pi^{SFT}$ is the reference policy (frozen SFT model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jrF8ybakOK5",
        "outputId": "eb8764e5-4afb-42fd-db51-b0c0c39e9fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value head device: cuda:0\n",
            "Value head dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "# Adding these so that types match up correctly\n",
        "\n",
        "# Move the value head to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.v_head.to(device)\n",
        "\n",
        "# Verify it works\n",
        "print(f\"Value head device: {next(model.v_head.parameters()).device}\")\n",
        "\n",
        "# Convert the value head to float16 to match the base model's output\n",
        "model.v_head.to(dtype=torch.bfloat16)\n",
        "\n",
        "# Verify the fix\n",
        "print(f\"Value head dtype: {model.v_head.value.weight.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW8X18-Mlb3v",
        "outputId": "4d5c85ce-5414-4fc1-849b-9b0ecdc3b805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating reference model (this may take a moment for 7B model)...\n",
            "✓ Reference model created\n"
          ]
        }
      ],
      "source": [
        "# Create reference model (frozen copy for KL divergence)\n",
        "# NOTE: For large models like Zephyr 7B, deepcopy might use a lot of memory\n",
        "# The reference model stays frozen during training\n",
        "from copy import deepcopy\n",
        "print(\"Creating reference model (this may take a moment for 7B model)...\")\n",
        "sft_model = deepcopy(model)\n",
        "sft_model.eval()  # Set to evaluation mode (frozen)\n",
        "print(\"✓ Reference model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFAE8zQPT-uP"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hi6IhpByUWWq",
        "outputId": "b41fc9d2-4a61-4646-95b8-62b26dae14d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  523, 28766,  1838, 28766, 28767,    13,   657,   456,  3638, 28725,\n",
              "           368,   460,  2078,   989, 23748, 28723,  3604,  3638,   349,   298,\n",
              "           875,  1575,   272,  2078, 23748,   390,   345,  5613, 28739,   513,\n",
              "           590,   506,  1348,  5746, 28745,  5860, 28725,   875,  1575,   706,\n",
              "           390,   345,  2501,  2586, 28705,    13,    13, 20275, 11232, 28747,\n",
              "           318,   308,   636, 28733, 28740, 28747,   315, 28742,   333,  1484,\n",
              "         10930,   778,   706, 26364, 21571, 28767, 26968,   636, 28733, 28750,\n",
              "         28747,  5410, 12076, 18922,   842,    13, 20275, 15985, 28747,  1770,\n",
              "            13,    13, 20275, 11232, 28747,   318,   308,   636, 28733, 28740,\n",
              "         28747,   315,  3088,   582, 23573,   739,   315,   947,   298, 26364,\n",
              "         21571, 28767, 26968,   636, 28733, 28750, 28747,   315,   771,   438,\n",
              "           272,  1819,  2827,   842,    13, 20275, 15985, 28747,  5592,    13,\n",
              "            13, 20275, 11232, 28747,   318,   308,   636, 28733, 28740, 28747,\n",
              "           816,   460,  1309,  5345, 26364, 21571, 28767, 26968,   636, 28733,\n",
              "         28750, 28747,   315,   863,   625,   586,   633,  4007,   842,    13,\n",
              "         20275, 15985, 28747,     2, 28705,    13, 28789, 28766,   489, 11143,\n",
              "         28766, 28767,    13,  2501,    13,    13, 20275, 11232, 28747,   318,\n",
              "           308,   636, 28733, 28740, 28747,  3489,  2496,  5751,  4154,  7034,\n",
              "         26364, 21571, 28767, 26968,   636, 28733, 28750, 28747,  3489,  5084,\n",
              "         21415,  4392,  4154,   842,    13, 20275, 15985, 28747,  5592,    13,\n",
              "            13, 20275, 11232, 28747,   318,   308,   636, 28733, 28740, 28747,\n",
              "          1984,  1253,  4312, 15667,  1060, 26364, 21571, 28767, 26968,   636,\n",
              "         28733, 28750, 28747,   315, 11220,   506,  1253, 21477,   842,    13,\n",
              "         20275, 15985, 28747,  5592,    13,    13, 20275, 11232, 28747,   318,\n",
              "           308,   636, 28733, 28740, 28747,   315,  2016,  4865, 20302,  1159,\n",
              "           590,   460,  5242, 26364, 21571],\n",
              "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,   523, 28766,  1838,\n",
              "         28766, 28767,    13,  5238,   264, 21366,  2007,   369, 21415,   264,\n",
              "          2245,  1729,   390,  2787,   304, 10148, 12189,   378, 28725,  9131,\n",
              "           288,   272,  1080,  8598, 28049,   477,   272,  2245, 28723,   415,\n",
              "          2007,  1023,  4681, 23175,   272,  1080,  3298,   304, 19258,  3085,\n",
              "         28725,  1259,   390,   307,  1068, 28713,   304,  1429,  1816, 28725,\n",
              "           304,  9303,  2115,  3085,   737,   345,  1237, 28739,   442,   345,\n",
              "           391,  2586,   415, 25081, 28049,  1023,   347, 17952,   486, 11010,\n",
              "           304, 13992,   298,   272,  2188, 28723, 16569, 28725,   272,  2007,\n",
              "          1023,  2405,   396, 11284,  5621,   298,  3024,   442, 25482,  2948,\n",
              "          3085,   442, 27512,   477,   272,  5643, 28723,     2, 28705,    13,\n",
              "         28789, 28766,   489, 11143, 28766, 28767,    13,  1551,  2231,  1259,\n",
              "           264,  2007, 28725,   368,   541, 22535,   272, 16725, 15589, 12877,\n",
              "         11153,   325, 28711,  2244, 28729, 28731,  7607,   297, 21366, 28723,\n",
              "          4003, 28742, 28713,   396,  2757,  9786, 28747,    13,    13, 13940,\n",
              "         28832, 17667,    13,  7841,   307,  2244, 28729,    13,  3211,   307,\n",
              "          2244, 28729, 28723,  4754, 20272]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data = data_collator([\n",
        "    {'input_ids': ids,\n",
        "     'attention_mask': torch.ones_like(ids)} for ids in query_response_tensors\n",
        "]).to(device)\n",
        "input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxbbEH4eX4Ab"
      },
      "outputs": [],
      "source": [
        "def compute_rewards(input_data, query_tensors, response_tensors, score_tensors):\n",
        "    with torch.no_grad():\n",
        "        logits, values = model(**input_data) # b, seq, vocab\n",
        "        ref_logits, _ = sft_model(**input_data)\n",
        "\n",
        "        # FIX 1: Clamp logits to avoid -inf/nan in log_softmax\n",
        "        logits = torch.clamp(logits, min=-100, max=100)\n",
        "        ref_logits = torch.clamp(ref_logits, min=-100, max=100)\n",
        "\n",
        "        logp = torch.nn.functional.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "        ref_logp = torch.nn.functional.log_softmax(ref_logits[:, :-1, :], dim=-1)\n",
        "\n",
        "        # FIX 2: These lines were missing! They select the prob of the actual token used.\n",
        "        labels = input_data['input_ids'][:, 1:] # b, seq\n",
        "        logp = torch.gather(logp, 2, labels.unsqueeze(-1)).squeeze(-1) # batch, seq\n",
        "        ref_logp = torch.gather(ref_logp, 2, labels.unsqueeze(-1)).squeeze(-1) # batch, seq\n",
        "\n",
        "        kl = logp - ref_logp\n",
        "        beta = 0.2\n",
        "        rewards = - beta * kl\n",
        "        attention_mask = input_data['attention_mask']\n",
        "        masks = torch.zeros_like(attention_mask[:, 1:])\n",
        "        masks[:,:] = attention_mask[:, 1:]\n",
        "\n",
        "        for j in range(len(query_tensors)):\n",
        "            start = len(query_tensors[j]) - 1\n",
        "            end = start + len(response_tensors[j])\n",
        "            masks[j, :start] = 0\n",
        "            masks[j, end:] = 0\n",
        "            rewards[j, end - 1] += score_tensors[j]\n",
        "            rewards[j, :] *= masks[j, :]\n",
        "            values[j, :-1] *= masks[j, :]\n",
        "\n",
        "    return logp, rewards, values[:, :-1], masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy9FJ_jFcyFC"
      },
      "outputs": [],
      "source": [
        "logprobs, rewards, values, masks = compute_rewards(input_data, query_tensors, response_tensors, score_tensors)\n",
        "# print(rewards[0])\n",
        "# print(input_data['input_ids'][0])\n",
        "# print(input_data['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBHebohxdFR5",
        "outputId": "063d6094-50a8-4c02-92d1-60f30d618cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([  0.0000,  -0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,  -0.0000,\n",
            "         -0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,  -0.0000,\n",
            "          0.0000,   0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,\n",
            "          0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,\n",
            "         -0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,\n",
            "          0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,  -0.0000,\n",
            "          0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,  -0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,  -0.0000,\n",
            "          0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,   0.0000,  -0.0000,\n",
            "         -0.0000,   0.0000,  -0.0000,   0.0000,  -0.0000,   0.0000,   0.0000,\n",
            "         -0.0000,  -0.0000,   0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,\n",
            "         -0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,\n",
            "          0.0000,  -0.0000,  -0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,\n",
            "         -0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,   0.0000,\n",
            "         -0.0000,   0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,  -3.6406,  14.7500,  -0.4434,  -3.9844,   9.1875,  -2.4219,\n",
            "          5.2812,  -0.3281,   6.1250,  -4.5625, -16.3750,  -3.0938,   7.1250,\n",
            "         -4.3750,  -0.5352,  -7.5625,   0.6562,   3.7344,  -7.2500,  -8.0000,\n",
            "         -5.2500,   6.0938,   2.0312,  -0.2695,   0.6445,   6.3438,   1.1797,\n",
            "         -1.6250,  -3.1875,   0.3652,  -7.0625,   5.2812,  -1.5938,  13.1250,\n",
            "         -0.5781,  12.3750,   6.7188,  -2.3750,  -6.1562,   8.6250,  -6.2500,\n",
            "          2.6406,   1.0156,   5.8125,  -5.4688, -10.3125,  -1.8984,   7.4062,\n",
            "         -2.2500,  -4.4375,   2.0000,   5.9062,   1.4531,  -1.5703,  -8.5625,\n",
            "         -8.7500,   6.5312,  -0.8047,   0.5508,   0.8906,   4.4062,  -5.6250,\n",
            "         -4.3750,   4.7500,   3.4375,   1.1406,   3.0938,  -2.4844,  12.5000,\n",
            "          0.0476,  11.6250,   3.2031,  -2.7812,  -6.6250,   7.5625,  -7.6562,\n",
            "          1.6016,   1.1562,   7.9375,  -4.1562,  -8.8750,  -2.5312,   5.5000,\n",
            "         -6.5938,  -8.6875,   5.0938,  -1.8672,   6.4688,   0.2617,  -3.5938,\n",
            "          2.1250,  -4.8125], device='cuda:0', dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "print(masks[0])\n",
        "print(values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnlh9TEeH-h"
      },
      "source": [
        "## Compute Advantage\n",
        "\n",
        "The advantage function estimates how much better an action is compared to the average:\n",
        "- **Positive advantage**: This action is better than expected\n",
        "- **Negative advantage**: This action is worse than expected\n",
        "\n",
        "Uses Generalized Advantage Estimation (GAE) for lower variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rL-IzH8eVZN"
      },
      "outputs": [],
      "source": [
        "def masked_mean(values, mask):\n",
        "    return (values * mask).sum() / mask.sum()\n",
        "\n",
        "def masked_var(values, mask):\n",
        "    mean = masked_mean(values, mask)\n",
        "    centred_values = values - mean\n",
        "    return masked_mean(centred_values ** 2, mask)\n",
        "\n",
        "# def masked_whiten(values, mask):\n",
        "#     mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
        "#     whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
        "#     whitened += mean\n",
        "#     return whitened\n",
        "def masked_whiten(values, mask):\n",
        "    mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
        "    # Add a larger epsilon (1e-6 instead of 1e-8) for stability\n",
        "    # If variance is 0, just return the centered values without scaling\n",
        "    if var < 1e-6:\n",
        "        return values - mean\n",
        "    whitened = (values - mean) * torch.rsqrt(var + 1e-6)\n",
        "    return whitened  # Note: Standard whitening returns (x-mu)/sigma, not adding mean back\n",
        "\n",
        "def compute_advantage(rewards, values, masks):\n",
        "    lastgae = 0.0\n",
        "    advantage_reversed = []\n",
        "    seq_length = rewards.shape[-1]\n",
        "    gamma, lam = 1.0, 0.95\n",
        "\n",
        "    for t in reversed(range(seq_length)):\n",
        "        nextvalues = values[:, t + 1] if t < seq_length - 1 else 0.0\n",
        "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
        "        lastgae = delta + gamma * lam * lastgae\n",
        "        advantage_reversed.append(lastgae)\n",
        "    advantages = torch.stack(advantage_reversed[::-1], dim=1)\n",
        "    advantages = masked_whiten(advantages, masks)\n",
        "\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ZTjweyijUu",
        "outputId": "9ced23c8-765c-406e-bbd6-8aa849196f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.2266e-02,\n",
            "        -7.2266e-02, -7.2266e-02, -7.2266e-02, -7.1777e-02, -7.1777e-02,\n",
            "        -7.1777e-02, -7.1777e-02, -7.1777e-02, -7.1777e-02, -7.1777e-02,\n",
            "        -7.1777e-02, -7.1777e-02, -7.1777e-02, -7.1289e-02, -7.1289e-02,\n",
            "        -7.1289e-02, -7.1289e-02, -7.1289e-02, -7.1289e-02, -7.1289e-02,\n",
            "        -7.1289e-02, -7.1289e-02, -7.1289e-02, -7.1289e-02, -7.0801e-02,\n",
            "        -7.0801e-02, -7.0801e-02, -7.0801e-02, -7.0312e-02, -7.0312e-02,\n",
            "        -7.0312e-02, -7.0312e-02, -7.0312e-02, -7.0312e-02, -6.9824e-02,\n",
            "        -6.9824e-02, -6.9336e-02, -6.9336e-02, -6.9336e-02, -6.9336e-02,\n",
            "        -6.9336e-02, -6.8848e-02, -6.8359e-02, -6.8359e-02, -6.8359e-02,\n",
            "        -6.7871e-02, -6.7871e-02, -6.7383e-02, -6.7383e-02, -6.6895e-02,\n",
            "        -6.6406e-02, -6.6406e-02, -6.5918e-02, -6.5918e-02, -6.4941e-02,\n",
            "        -6.4941e-02, -6.4453e-02, -6.3965e-02, -6.3965e-02, -6.2988e-02,\n",
            "        -6.2988e-02, -6.2012e-02, -6.1768e-02, -6.1035e-02, -6.0547e-02,\n",
            "        -5.9814e-02, -5.9082e-02, -5.8350e-02, -5.7617e-02, -5.7129e-02,\n",
            "        -5.6152e-02, -5.5176e-02, -5.4443e-02, -5.3467e-02, -5.2246e-02,\n",
            "        -5.1270e-02, -5.0293e-02, -4.9072e-02, -4.7607e-02, -4.6387e-02,\n",
            "        -4.4922e-02, -4.3701e-02, -4.2236e-02, -4.0527e-02, -3.8818e-02,\n",
            "        -3.6865e-02, -3.5156e-02, -3.2959e-02, -3.1006e-02, -2.8809e-02,\n",
            "        -2.6489e-02, -2.4170e-02,  6.3672e-01, -2.6562e+00, -4.0283e-02,\n",
            "         6.0547e-01, -1.7500e+00,  2.6562e-01, -1.1094e+00, -1.4746e-01,\n",
            "        -1.3203e+00,  5.5078e-01,  2.7188e+00,  4.6484e-01, -1.3672e+00,\n",
            "         6.5234e-01, -2.6855e-03,  1.2812e+00, -1.4453e-01, -7.1094e-01,\n",
            "         1.2500e+00,  1.4531e+00,  1.0312e+00, -9.6875e-01, -2.7930e-01,\n",
            "         1.2500e-01, -2.9541e-02, -1.0547e+00, -1.7480e-01,  3.2812e-01,\n",
            "         6.3281e-01,  2.4170e-02,  1.3750e+00, -7.8516e-01,  4.2188e-01,\n",
            "        -2.2188e+00,  1.4258e-01, -2.1875e+00, -1.2812e+00,  3.0859e-01,\n",
            "         1.0078e+00, -1.6016e+00,  1.0234e+00, -5.2734e-01, -2.5977e-01,\n",
            "        -1.1406e+00,  8.3594e-01,  1.7656e+00,  3.3594e-01, -1.3281e+00,\n",
            "         3.4570e-01,  7.6172e-01, -3.5742e-01, -1.0781e+00, -3.2812e-01,\n",
            "         2.0703e-01,  1.4844e+00,  1.6016e+00, -1.0625e+00,  2.1875e-01,\n",
            "        -1.1414e-02, -6.9824e-02, -7.1094e-01,  1.0781e+00,  9.0234e-01,\n",
            "        -6.9922e-01, -4.9414e-01, -1.0205e-01, -4.5703e-01,  5.2734e-01,\n",
            "        -2.1562e+00, -1.8799e-02, -2.1094e+00, -6.9922e-01,  3.5156e-01,\n",
            "         1.0781e+00, -1.4375e+00,  1.2500e+00, -3.5938e-01, -2.9297e-01,\n",
            "        -1.5391e+00,  5.7812e-01,  1.4609e+00,  3.9453e-01, -1.0312e+00,\n",
            "         1.1172e+00,  1.5547e+00, -8.3594e-01,  3.8867e-01, -1.0938e+00,\n",
            "        -2.1484e-02,  6.7969e-01, -3.1445e-01,  9.2188e-01], device='cuda:0',\n",
            "       dtype=torch.bfloat16)\n",
            "tensor([ -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,  -0.0723,\n",
            "         -0.0723,  -0.0723,  -0.0723,  -0.0718,  -0.0718,  -0.0718,  -0.0718,\n",
            "         -0.0718,  -0.0718,  -0.0718,  -0.0718,  -0.0718,  -0.0718,  -0.0713,\n",
            "         -0.0713,  -0.0713,  -0.0713,  -0.0713,  -0.0713,  -0.0713,  -0.0713,\n",
            "         -0.0713,  -0.0713,  -0.0713,  -0.0708,  -0.0708,  -0.0708,  -0.0708,\n",
            "         -0.0703,  -0.0703,  -0.0703,  -0.0703,  -0.0703,  -0.0703,  -0.0698,\n",
            "         -0.0698,  -0.0693,  -0.0693,  -0.0693,  -0.0693,  -0.0693,  -0.0688,\n",
            "         -0.0684,  -0.0684,  -0.0684,  -0.0679,  -0.0679,  -0.0674,  -0.0674,\n",
            "         -0.0669,  -0.0664,  -0.0664,  -0.0659,  -0.0659,  -0.0649,  -0.0649,\n",
            "         -0.0645,  -0.0640,  -0.0640,  -0.0630,  -0.0630,  -0.0620,  -0.0618,\n",
            "         -0.0610,  -0.0605,  -0.0598,  -0.0591,  -0.0583,  -0.0576,  -0.0571,\n",
            "         -0.0562,  -0.0552,  -0.0544,  -0.0535,  -0.0522,  -0.0513,  -0.0503,\n",
            "         -0.0491,  -0.0476,  -0.0464,  -0.0449,  -0.0437,  -0.0422,  -0.0405,\n",
            "         -0.0388,  -0.0369,  -0.0352,  -0.0330,  -0.0310,  -0.0288,  -0.0265,\n",
            "         -0.0242,  -3.0000,  12.1250,  -0.4844,  -3.3750,   7.4375,  -2.1562,\n",
            "          4.1875,  -0.4766,   4.8125,  -4.0000, -13.6250,  -2.6250,   5.7500,\n",
            "         -3.7188,  -0.5391,  -6.2812,   0.5117,   3.0312,  -6.0000,  -6.5625,\n",
            "         -4.2188,   5.1250,   1.7500,  -0.1445,   0.6133,   5.2812,   1.0078,\n",
            "         -1.2969,  -2.5625,   0.3887,  -5.6875,   4.5000,  -1.1719,  10.8750,\n",
            "         -0.4355,  10.1875,   5.4375,  -2.0625,  -5.1562,   7.0312,  -5.2188,\n",
            "          2.1094,   0.7578,   4.6875,  -4.6250,  -8.5625,  -1.5625,   6.0625,\n",
            "         -1.9062,  -3.6719,   1.6406,   4.8125,   1.1250,  -1.3594,  -7.0625,\n",
            "         -7.1562,   5.4688,  -0.5859,   0.5391,   0.8203,   3.6875,  -4.5625,\n",
            "         -3.4688,   4.0625,   2.9375,   1.0391,   2.6406,  -1.9531,  10.3750,\n",
            "          0.0288,   9.5000,   2.5000,  -2.4375,  -5.5625,   6.1250,  -6.4062,\n",
            "          1.2422,   0.8633,   6.4062,  -3.5781,  -7.4062,  -2.1406,   4.4688,\n",
            "         -5.4688,  -7.1250,   4.2500,  -1.4766,   5.3750,   0.2402,  -2.9062,\n",
            "          1.8125,  -3.8906], device='cuda:0', dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "advantages, returns = compute_advantage(rewards, values, masks)\n",
        "print(advantages[0])\n",
        "print(returns[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dACSgd1ripo_"
      },
      "source": [
        "## Mini-batch PPO Training\n",
        "\n",
        "PPO updates the policy using mini-batches to improve sample efficiency and stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTTWJrvLxaUz"
      },
      "source": [
        "### Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsqMGuocxdJr",
        "outputId": "221677c3-8ed5-4688-f6bb-46e094ab10a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer created with lr=1e-05\n"
          ]
        }
      ],
      "source": [
        "# Training hyperparameters\n",
        "# NOTE: These may need adjustment for Zephyr 7B\n",
        "learning_rate = 1e-5  # Conservative learning rate for fine-tuning\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "print(f\"Optimizer created with lr={learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwYclizbyNg1",
        "outputId": "76ee2b94-5c20-46b5-9466-8ae09dc4b192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.permutation(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhiRjkGCzBLI",
        "outputId": "bb4187c7-c695-4e69-ff17-41f879ebab32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO config: 4 epochs, mini_batch_size=2\n"
          ]
        }
      ],
      "source": [
        "# PPO training configuration\n",
        "mini_batch_size = 2  # Reduced from 4 for memory efficiency with 7B model\n",
        "ppo_epochs = 4\n",
        "\n",
        "cliprange_ratio = 0.2  # PPO clipping range\n",
        "v_loss_coeff = 0.1     # Value loss coefficient\n",
        "ratio_threshold = 10   # Threshold to detect unstable training\n",
        "\n",
        "def compute_loss(old_logprobs, values, logprobs, vpreds, masks, advantages, returns):\n",
        "    \"\"\"\n",
        "    Compute PPO loss with clipping.\n",
        "    \"\"\"\n",
        "    ratio = torch.exp(logprobs - old_logprobs)\n",
        "    pg_loss1 = - ratio * advantages\n",
        "    pg_loss2 = - torch.clamp(ratio, 1 - cliprange_ratio, 1 + cliprange_ratio) * advantages\n",
        "    pg_loss = masked_mean(torch.max(pg_loss1, pg_loss2), masks)\n",
        "\n",
        "    v_loss = masked_mean((vpreds - returns) ** 2, masks)\n",
        "    loss = pg_loss + v_loss_coeff * v_loss\n",
        "\n",
        "    avg_ratio = masked_mean(ratio, masks)\n",
        "    if avg_ratio > ratio_threshold:\n",
        "        # Unstable training detected - zero out gradients\n",
        "        pg_loss = pg_loss * 0.0\n",
        "        v_loss = v_loss * 0.0\n",
        "        loss = loss * 0.0\n",
        "\n",
        "    return loss, v_loss\n",
        "\n",
        "def mini_batch_train():\n",
        "    \"\"\"Run mini-batch PPO training for multiple epochs.\"\"\"\n",
        "    # FIX 1: Get actual batch size from input data (prevents index out of bounds)\n",
        "    current_batch_size = len(input_data['input_ids'])\n",
        "\n",
        "    for ep in range(ppo_epochs):\n",
        "        # FIX 1: Use current_batch_size for permutation\n",
        "        batch_inds = np.random.permutation(current_batch_size)\n",
        "\n",
        "        for start in range(0, current_batch_size, mini_batch_size):\n",
        "            end = start + mini_batch_size\n",
        "            mini_batch_inds = batch_inds[start:end]\n",
        "\n",
        "            mb_model_inputs = {\n",
        "                'input_ids': input_data['input_ids'][mini_batch_inds],\n",
        "                'attention_mask': input_data['attention_mask'][mini_batch_inds]\n",
        "            }\n",
        "            mb_logits, mb_vpreds = model(**mb_model_inputs)\n",
        "\n",
        "            # FIX 2: Clamp logits here to prevent NaN loss!\n",
        "            mb_logits = torch.clamp(mb_logits, min=-100, max=100)\n",
        "\n",
        "            mb_logits = torch.nn.functional.log_softmax(mb_logits[:, :-1, :], dim=-1)\n",
        "            mb_logprobs = torch.gather(mb_logits, 2, mb_model_inputs['input_ids'][:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            loss, loss_v = compute_loss(\n",
        "                logprobs[mini_batch_inds],\n",
        "                values[mini_batch_inds],\n",
        "                mb_logprobs,\n",
        "                mb_vpreds[:, :-1],\n",
        "                masks[mini_batch_inds],\n",
        "                advantages[mini_batch_inds],\n",
        "                returns[mini_batch_inds]\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (you already had this, keep it!)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Debug print\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"WARNING: Loss is NaN in epoch {ep}!\")\n",
        "            else:\n",
        "                print('loss/total', loss.item())\n",
        "    print('mini-batch training finished')\n",
        "\n",
        "print(f\"PPO config: {ppo_epochs} epochs, mini_batch_size={mini_batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TmyCw7g9zGU",
        "outputId": "453c784a-10b4-4c45-d481-075d4ff5742b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss/total 0.10107421875\n",
            "loss/total 0.06640625\n",
            "loss/total 0.033203125\n",
            "loss/total 0.01806640625\n",
            "mini-batch training finished\n"
          ]
        }
      ],
      "source": [
        "mini_batch_train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcZPLHlb91la"
      },
      "source": [
        "## Train RLHF\n",
        "\n",
        "The main training loop:\n",
        "1. Generate responses for each prompt\n",
        "2. Score responses with LLM judge\n",
        "3. Compute rewards (score - KL penalty)\n",
        "4. Compute advantages (how good was this response?)\n",
        "5. Update the policy with PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsh0CnDR-zyN",
        "outputId": "de309a92-5895-4669-c2b9-39e9890a5427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss/total 0.1015625\n",
            "loss/total 0.059814453125\n",
            "loss/total 0.03369140625\n",
            "loss/total 0.0126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.06689453125\n",
            "loss/total 0.04248046875\n",
            "loss/total 0.02099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.083984375\n",
            "loss/total 0.059326171875\n",
            "loss/total 0.044189453125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.07958984375\n",
            "loss/total 0.05859375\n",
            "loss/total 0.030029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.072265625\n",
            "loss/total 0.044189453125\n",
            "loss/total 0.01220703125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.072265625\n",
            "loss/total 0.04296875\n",
            "loss/total 0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06640625\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.019287109375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.033447265625\n",
            "loss/total 0.0146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07421875\n",
            "loss/total 0.048828125\n",
            "loss/total 0.03125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.01953125\n",
            "loss/total -0.03076171875\n",
            "loss/total -0.0693359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.044189453125\n",
            "loss/total -0.001708984375\n",
            "loss/total -0.007080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.029541015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0537109375\n",
            "loss/total 0.00390625\n",
            "loss/total -0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0556640625\n",
            "loss/total 0.03076171875\n",
            "loss/total 0.00830078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.064453125\n",
            "loss/total 0.04736328125\n",
            "loss/total 0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04638671875\n",
            "loss/total 0.0185546875\n",
            "loss/total -0.00390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.05224609375\n",
            "loss/total 0.03662109375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.0556640625\n",
            "loss/total 0.03857421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.068359375\n",
            "loss/total 0.049560546875\n",
            "loss/total 0.03076171875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06201171875\n",
            "loss/total 0.02587890625\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.029541015625\n",
            "loss/total 0.00927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.064453125\n",
            "loss/total 0.0400390625\n",
            "loss/total 0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.037841796875\n",
            "loss/total 0.00830078125\n",
            "loss/total -0.014892578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.0517578125\n",
            "loss/total 0.018310546875\n",
            "loss/total -0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.032470703125\n",
            "loss/total 0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.00439453125\n",
            "loss/total -0.02099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.061767578125\n",
            "loss/total 0.042724609375\n",
            "loss/total 0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.009765625\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.052490234375\n",
            "loss/total 0.026611328125\n",
            "loss/total 0.00634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.053466796875\n",
            "loss/total 0.022705078125\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.01904296875\n",
            "loss/total -0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.059814453125\n",
            "loss/total 0.02783203125\n",
            "loss/total 0.0068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.07421875\n",
            "loss/total 0.051513671875\n",
            "loss/total 0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.056396484375\n",
            "loss/total 0.02685546875\n",
            "loss/total 0.006103515625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05078125\n",
            "loss/total 0.01513671875\n",
            "loss/total -0.0087890625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.068359375\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.027099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.048095703125\n",
            "loss/total 0.02001953125\n",
            "loss/total -0.00537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.052978515625\n",
            "loss/total 0.0283203125\n",
            "loss/total 0.0126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.0595703125\n",
            "loss/total 0.04150390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.02880859375\n",
            "loss/total -0.00146484375\n",
            "loss/total -0.0244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.056884765625\n",
            "loss/total 0.026123046875\n",
            "loss/total 0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.045166015625\n",
            "loss/total -0.0009765625\n",
            "loss/total -0.02734375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.06103515625\n",
            "loss/total 0.0390625\n",
            "loss/total 0.02099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.035400390625\n",
            "loss/total 0.00701904296875\n",
            "loss/total 0.0189208984375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.02294921875\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.0205078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06103515625\n",
            "loss/total 0.035400390625\n",
            "loss/total 0.013427734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.037109375\n",
            "loss/total 0.00341796875\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.048583984375\n",
            "loss/total 0.027099609375\n",
            "loss/total 0.0078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.068359375\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.018310546875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.060546875\n",
            "loss/total 0.032470703125\n",
            "loss/total 0.00537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05419921875\n",
            "loss/total 0.020751953125\n",
            "loss/total -0.005126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.0087890625\n",
            "loss/total -0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.07421875\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.0146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.021240234375\n",
            "loss/total -0.00341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.005859375\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.021484375\n",
            "loss/total -0.01806640625\n",
            "loss/total -0.04296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.00341796875\n",
            "loss/total -0.021240234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.048095703125\n",
            "loss/total 0.01904296875\n",
            "loss/total -0.01953125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.042724609375\n",
            "loss/total 0.00244140625\n",
            "loss/total -0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.02685546875\n",
            "loss/total -0.021484375\n",
            "loss/total -0.0439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.043212890625\n",
            "loss/total 0.0208740234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0625\n",
            "loss/total 0.036376953125\n",
            "loss/total 0.01953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total -0.0078125\n",
            "loss/total -0.05615234375\n",
            "loss/total -0.062255859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.053955078125\n",
            "loss/total 0.033935546875\n",
            "loss/total 0.0126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.031005859375\n",
            "loss/total -0.0166015625\n",
            "loss/total -0.043212890625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.053466796875\n",
            "loss/total 0.012451171875\n",
            "loss/total -0.031005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.064453125\n",
            "loss/total 0.03271484375\n",
            "loss/total 0.001220703125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.040771484375\n",
            "loss/total 0.00439453125\n",
            "loss/total -0.01806640625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.041015625\n",
            "loss/total 0.02001953125\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03076171875\n",
            "loss/total -0.012451171875\n",
            "loss/total -0.0361328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.034912109375\n",
            "loss/total 0.01806640625\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03076171875\n",
            "loss/total -0.01318359375\n",
            "loss/total -0.0301513671875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.043212890625\n",
            "loss/total 0.0087890625\n",
            "loss/total -0.0224609375\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.02197265625\n",
            "loss/total -0.0087890625\n",
            "loss/total -0.038330078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.05908203125\n",
            "loss/total 0.0283203125\n",
            "loss/total 0.00146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.02001953125\n",
            "loss/total -0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.05224609375\n",
            "loss/total 0.039306640625\n",
            "loss/total 0.0166015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.0146484375\n",
            "loss/total -0.00927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total -0.02099609375\n",
            "loss/total -0.0693359375\n",
            "loss/total -0.0947265625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06884765625\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.0244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.051025390625\n",
            "loss/total 0.01513671875\n",
            "loss/total 0.0009765625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total -0.0009765625\n",
            "loss/total -0.0341796875\n",
            "loss/total -0.05517578125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total -0.009765625\n",
            "loss/total -0.05419921875\n",
            "loss/total -0.08056640625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.009765625\n",
            "loss/total -0.0263671875\n",
            "loss/total -0.05029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.023193359375\n",
            "loss/total -0.00341796875\n",
            "loss/total -0.033935546875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0380859375\n",
            "loss/total 0.009765625\n",
            "loss/total -0.0224609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.029296875\n",
            "loss/total -0.0029296875\n",
            "loss/total -0.02783203125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.033935546875\n",
            "loss/total 0.00341796875\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.051025390625\n",
            "loss/total 0.024658203125\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0341796875\n",
            "loss/total -0.0087890625\n",
            "loss/total -0.0458984375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.02197265625\n",
            "loss/total -0.01318359375\n",
            "loss/total -0.038818359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0703125\n",
            "loss/total 0.039306640625\n",
            "loss/total 0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.029541015625\n",
            "loss/total 0.0\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0400390625\n",
            "loss/total 0.017578125\n",
            "loss/total -0.00390625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.030029296875\n",
            "loss/total 0.001708984375\n",
            "loss/total -0.02294921875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04150390625\n",
            "loss/total 0.015625\n",
            "loss/total -0.0146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.044921875\n",
            "loss/total 0.010986328125\n",
            "loss/total -0.0078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.035400390625\n",
            "loss/total 0.005859375\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.1240234375\n",
            "loss/total 0.02783203125\n",
            "loss/total 0.0048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04736328125\n",
            "loss/total 0.0126953125\n",
            "loss/total -0.0107421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.047607421875\n",
            "loss/total 0.025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0439453125\n",
            "loss/total 0.01953125\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.059814453125\n",
            "loss/total 0.029541015625\n",
            "loss/total 0.004638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0302734375\n",
            "loss/total 0.0029296875\n",
            "loss/total -0.023681640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.033447265625\n",
            "loss/total 0.00927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.009765625\n",
            "loss/total -0.01513671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.057861328125\n",
            "loss/total 0.0361328125\n",
            "loss/total 0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.029296875\n",
            "loss/total -0.0283203125\n",
            "loss/total -0.060546875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.01123046875\n",
            "loss/total -0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.01318359375\n",
            "loss/total -0.01513671875\n",
            "loss/total -0.04150390625\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.02685546875\n",
            "loss/total -0.01806640625\n",
            "loss/total -0.041015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.072265625\n",
            "loss/total 0.046630859375\n",
            "loss/total 0.0274658203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0244140625\n",
            "loss/total -0.01318359375\n",
            "loss/total -0.037109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0361328125\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.052734375\n",
            "loss/total 0.01708984375\n",
            "loss/total -0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.0263671875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.03515625\n",
            "loss/total 0.00927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.031982421875\n",
            "loss/total 0.0126953125\n",
            "loss/total -0.0048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.03564453125\n",
            "loss/total 0.001953125\n",
            "loss/total -0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.024169921875\n",
            "loss/total -0.01708984375\n",
            "loss/total -0.04345703125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0380859375\n",
            "loss/total 0.005859375\n",
            "loss/total -0.022216796875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.036376953125\n",
            "loss/total 0.00537109375\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.04443359375\n",
            "loss/total 0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.046875\n",
            "loss/total 0.01123046875\n",
            "loss/total -0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.043701171875\n",
            "loss/total 0.008056640625\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0390625\n",
            "loss/total 0.00830078125\n",
            "loss/total -0.01171875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0146484375\n",
            "loss/total -0.021484375\n",
            "loss/total -0.048583984375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.00146484375\n",
            "loss/total -0.03662109375\n",
            "loss/total -0.068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.049072265625\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.00927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.033447265625\n",
            "loss/total -0.02734375\n",
            "loss/total -0.05419921875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0390625\n",
            "loss/total -0.00244140625\n",
            "loss/total -0.025634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05078125\n",
            "loss/total 0.018798828125\n",
            "loss/total -0.003662109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.061279296875\n",
            "loss/total 0.038330078125\n",
            "loss/total 0.015380859375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.040283203125\n",
            "loss/total 0.014892578125\n",
            "loss/total -0.006591796875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0380859375\n",
            "loss/total 0.0048828125\n",
            "loss/total -0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.025146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.013671875\n",
            "loss/total -0.0009765625\n",
            "loss/total -0.02734375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.029541015625\n",
            "loss/total 0.0\n",
            "loss/total -0.021240234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.040771484375\n",
            "loss/total 0.0185546875\n",
            "loss/total -0.00634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.052001953125\n",
            "loss/total -0.001953125\n",
            "loss/total -0.03173828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.0224609375\n",
            "loss/total 0.0078125\n",
            "loss/total -0.02587890625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.02392578125\n",
            "loss/total -0.02099609375\n",
            "loss/total -0.05078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.051025390625\n",
            "loss/total 0.0322265625\n",
            "loss/total 0.0087890625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.016845703125\n",
            "loss/total -0.00244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0166015625\n",
            "loss/total -0.01953125\n",
            "loss/total -0.046630859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.03955078125\n",
            "loss/total 0.00927734375\n",
            "loss/total -0.011474609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.03515625\n",
            "loss/total 0.0029296875\n",
            "loss/total -0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04833984375\n",
            "loss/total 0.022705078125\n",
            "loss/total 0.0\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.04296875\n",
            "loss/total 0.011474609375\n",
            "loss/total -0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.017822265625\n",
            "loss/total -0.00634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.055908203125\n",
            "loss/total 0.017333984375\n",
            "loss/total -0.0048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.056396484375\n",
            "loss/total 0.02685546875\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.03466796875\n",
            "loss/total 0.0068359375\n",
            "loss/total -0.0185546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.036865234375\n",
            "loss/total 0.00634765625\n",
            "loss/total -0.01513671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.02392578125\n",
            "loss/total -0.0146484375\n",
            "loss/total -0.041015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.04833984375\n",
            "loss/total 0.02197265625\n",
            "loss/total 0.0078125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.035888671875\n",
            "loss/total 0.00537109375\n",
            "loss/total -0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03125\n",
            "loss/total -0.019775390625\n",
            "loss/total -0.04638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.040283203125\n",
            "loss/total -0.00390625\n",
            "loss/total -0.041259765625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.052001953125\n",
            "loss/total 0.0224609375\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.05078125\n",
            "loss/total 0.019775390625\n",
            "loss/total -0.01025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.051513671875\n",
            "loss/total 0.022705078125\n",
            "loss/total -0.001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.02978515625\n",
            "loss/total -0.0068359375\n",
            "loss/total -0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.049072265625\n",
            "loss/total 0.021240234375\n",
            "loss/total -0.001708984375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.031494140625\n",
            "loss/total 0.016845703125\n",
            "loss/total 0.00244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.03662109375\n",
            "loss/total 0.0048828125\n",
            "loss/total -0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.036865234375\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.021484375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0185546875\n",
            "loss/total -0.01806640625\n",
            "loss/total -0.046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.045654296875\n",
            "loss/total 0.023681640625\n",
            "loss/total 0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0166015625\n",
            "loss/total -0.025146484375\n",
            "loss/total -0.05712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.00390625\n",
            "loss/total -0.03515625\n",
            "loss/total -0.0576171875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0234375\n",
            "loss/total -0.01025390625\n",
            "loss/total -0.033203125\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.025390625\n",
            "loss/total -0.00341796875\n",
            "loss/total -0.015869140625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.045166015625\n",
            "loss/total 0.01904296875\n",
            "loss/total 0.004150390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.054443359375\n",
            "loss/total 0.028076171875\n",
            "loss/total 0.002685546875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.042724609375\n",
            "loss/total 0.0\n",
            "loss/total -0.0185546875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.045654296875\n",
            "loss/total 0.016357421875\n",
            "loss/total -0.0048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.010009765625\n",
            "loss/total -0.0146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.038818359375\n",
            "loss/total 0.016357421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.02734375\n",
            "loss/total -0.005859375\n",
            "loss/total -0.02587890625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.064453125\n",
            "loss/total 0.0341796875\n",
            "loss/total 0.01220703125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.015625\n",
            "loss/total -0.00927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.046142578125\n",
            "loss/total 0.013671875\n",
            "loss/total -0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04443359375\n",
            "loss/total 0.01141357421875\n",
            "loss/total 0.01519775390625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.02734375\n",
            "loss/total 0.0107421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.033935546875\n",
            "loss/total 0.024658203125\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total -0.00146484375\n",
            "loss/total -0.047607421875\n",
            "loss/total -0.0751953125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.031982421875\n",
            "loss/total 0.0068359375\n",
            "loss/total -0.01611328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.005859375\n",
            "loss/total -0.030029296875\n",
            "loss/total -0.04296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04150390625\n",
            "loss/total 0.01513671875\n",
            "loss/total -0.00537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.058349609375\n",
            "loss/total 0.0240478515625\n",
            "loss/total -0.008544921875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.03662109375\n",
            "loss/total 0.0078125\n",
            "loss/total -0.0087890625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0224609375\n",
            "loss/total -0.01904296875\n",
            "loss/total -0.0439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.051513671875\n",
            "loss/total 0.030029296875\n",
            "loss/total 0.029052734375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.0263671875\n",
            "loss/total -0.048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.037841796875\n",
            "loss/total -0.014404296875\n",
            "loss/total -0.0361328125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.01416015625\n",
            "loss/total -0.002197265625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.03759765625\n",
            "loss/total -0.0654296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0380859375\n",
            "loss/total 0.00439453125\n",
            "loss/total -0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total -0.00244140625\n",
            "loss/total -0.04833984375\n",
            "loss/total -0.07666015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0859375\n",
            "loss/total 0.032958984375\n",
            "loss/total 0.005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0126953125\n",
            "loss/total -0.032470703125\n",
            "loss/total -0.06640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10546875\n",
            "loss/total -0.03515625\n",
            "loss/total -0.0634765625\n",
            "loss/total -0.0791015625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.021484375\n",
            "loss/total -0.01171875\n",
            "loss/total -0.04736328125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0576171875\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.00341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0146484375\n",
            "loss/total -0.0302734375\n",
            "loss/total -0.061767578125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total -0.00537109375\n",
            "loss/total -0.05615234375\n",
            "loss/total -0.08349609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.03662109375\n",
            "loss/total -0.0068359375\n",
            "loss/total -0.0361328125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.056884765625\n",
            "loss/total 0.03662109375\n",
            "loss/total 0.01708984375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.037109375\n",
            "loss/total 0.026123046875\n",
            "loss/total 0.033447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.047119140625\n",
            "loss/total 0.0361328125\n",
            "loss/total 0.04296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.064453125\n",
            "loss/total 0.02880859375\n",
            "loss/total 0.007080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.22265625\n",
            "loss/total 0.01220703125\n",
            "loss/total 0.0859375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.047119140625\n",
            "loss/total 0.03125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.41796875\n",
            "loss/total 0.2734375\n",
            "loss/total -0.0107421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06640625\n",
            "loss/total 0.03271484375\n",
            "loss/total 0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0\n",
            "loss/total -0.0361328125\n",
            "loss/total -0.0576171875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.01953125\n",
            "loss/total -0.01708984375\n",
            "loss/total 0.0068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.036376953125\n",
            "loss/total -0.0322265625\n",
            "loss/total -0.0673828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.00390625\n",
            "loss/total -0.0029296875\n",
            "loss/total 0.00390625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.02783203125\n",
            "loss/total -0.01904296875\n",
            "loss/total -0.05078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.02978515625\n",
            "loss/total -0.002197265625\n",
            "loss/total -0.03076171875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.01513671875\n",
            "loss/total -0.02099609375\n",
            "loss/total -0.03955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.02734375\n",
            "loss/total -0.01708984375\n",
            "loss/total -0.046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total -0.00634765625\n",
            "loss/total -0.02392578125\n",
            "loss/total -0.10546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.01904296875\n",
            "loss/total -0.0185546875\n",
            "loss/total -0.0380859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.03466796875\n",
            "loss/total -0.017578125\n",
            "loss/total -0.055419921875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.040283203125\n",
            "loss/total -0.01611328125\n",
            "loss/total -0.046875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0615234375\n",
            "loss/total 0.01220703125\n",
            "loss/total -0.01806640625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0712890625\n",
            "loss/total -0.01220703125\n",
            "loss/total -0.02587890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.036376953125\n",
            "loss/total 0.018310546875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04150390625\n",
            "loss/total -0.00634765625\n",
            "loss/total -0.02099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0263671875\n",
            "loss/total -0.0087890625\n",
            "loss/total -0.02392578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.00634765625\n",
            "loss/total -0.020263671875\n",
            "loss/total -0.031005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total -0.0078125\n",
            "loss/total -0.0986328125\n",
            "loss/total -0.0654296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.068359375\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.061279296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.03515625\n",
            "loss/total -0.0068359375\n",
            "loss/total -0.037109375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.00048828125\n",
            "loss/total -0.00927734375\n",
            "loss/total -0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.031005859375\n",
            "loss/total -0.0126953125\n",
            "loss/total -0.0517578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0166015625\n",
            "loss/total -0.029296875\n",
            "loss/total -0.0595703125\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total -0.0078125\n",
            "loss/total -0.05322265625\n",
            "loss/total -0.0712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0087890625\n",
            "loss/total -0.03564453125\n",
            "loss/total -0.0634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total -0.0146484375\n",
            "loss/total -0.0712890625\n",
            "loss/total -0.09521484375\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.028076171875\n",
            "loss/total -0.0068359375\n",
            "loss/total -0.03369140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.028076171875\n",
            "loss/total -0.017578125\n",
            "loss/total -0.060546875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.02197265625\n",
            "loss/total -0.01904296875\n",
            "loss/total -0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0\n",
            "loss/total -0.0302734375\n",
            "loss/total -0.05810546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.00830078125\n",
            "loss/total -0.0390625\n",
            "loss/total -0.05810546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.033935546875\n",
            "loss/total -0.025390625\n",
            "loss/total -0.05419921875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total -0.00830078125\n",
            "loss/total -0.04736328125\n",
            "loss/total -0.06640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total -0.01416015625\n",
            "loss/total -0.0634765625\n",
            "loss/total -0.08984375\n",
            "mini-batch training finished\n",
            "loss/total 0.10595703125\n",
            "loss/total 0.047607421875\n",
            "loss/total -0.01220703125\n",
            "loss/total -0.0498046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.02685546875\n",
            "loss/total -0.009765625\n",
            "loss/total -0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.033203125\n",
            "loss/total -0.0048828125\n",
            "loss/total -0.0302734375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.01025390625\n",
            "loss/total -0.044921875\n",
            "loss/total -0.07275390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.02197265625\n",
            "loss/total -0.016845703125\n",
            "loss/total -0.039794921875\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.020751953125\n",
            "loss/total -0.01953125\n",
            "loss/total -0.033447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.045166015625\n",
            "loss/total 0.0166015625\n",
            "loss/total -0.015869140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0234375\n",
            "loss/total -0.02587890625\n",
            "loss/total -0.04345703125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03564453125\n",
            "loss/total 0.003173828125\n",
            "loss/total -0.02001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.025390625\n",
            "loss/total 0.04833984375\n",
            "loss/total 0.2578125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.046630859375\n",
            "loss/total 0.0087890625\n",
            "loss/total 0.1435546875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0400390625\n",
            "loss/total 0.080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.037841796875\n",
            "loss/total 0.03515625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.060791015625\n",
            "loss/total 0.049560546875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.1240234375\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.00830078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.04541015625\n",
            "loss/total 0.028564453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.02099609375\n",
            "loss/total -0.0185546875\n",
            "loss/total -0.03564453125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0185546875\n",
            "loss/total -0.0146484375\n",
            "loss/total -0.0341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.028076171875\n",
            "loss/total 0.0498046875\n",
            "loss/total -0.0087890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0791015625\n",
            "loss/total -0.0009765625\n",
            "loss/total 0.033203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total -0.02685546875\n",
            "loss/total -0.0751953125\n",
            "loss/total -0.095703125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.03759765625\n",
            "loss/total 0.00341796875\n",
            "loss/total 0.005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0625\n",
            "loss/total 0.0087890625\n",
            "loss/total -0.01953125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.02490234375\n",
            "loss/total -0.03173828125\n",
            "loss/total -0.0400390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.0703125\n",
            "loss/total 0.053466796875\n",
            "loss/total 0.02734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.01953125\n",
            "loss/total -0.01708984375\n",
            "loss/total -0.03857421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total -0.00048828125\n",
            "loss/total -0.0390625\n",
            "loss/total -0.07958984375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.033203125\n",
            "loss/total -0.0048828125\n",
            "loss/total -0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.035400390625\n",
            "loss/total 0.019775390625\n",
            "loss/total 0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total -0.00390625\n",
            "loss/total -0.04052734375\n",
            "loss/total -0.0556640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.015625\n",
            "loss/total -0.029296875\n",
            "loss/total -0.04248046875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.07080078125\n",
            "loss/total -0.01171875\n",
            "loss/total -0.02392578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0048828125\n",
            "loss/total -0.03271484375\n",
            "loss/total -0.050048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.036865234375\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.04638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.02783203125\n",
            "loss/total 0.01953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total -0.0185546875\n",
            "loss/total -0.052734375\n",
            "loss/total -0.064453125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.01025390625\n",
            "loss/total -0.0556640625\n",
            "loss/total -0.078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.06884765625\n",
            "loss/total 0.03662109375\n",
            "loss/total 0.016357421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0146484375\n",
            "loss/total -0.01611328125\n",
            "loss/total -0.037109375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.041748046875\n",
            "loss/total -0.0126953125\n",
            "loss/total -0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.1240234375\n",
            "loss/total 0.1201171875\n",
            "loss/total 0.240234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10546875\n",
            "loss/total 0.0166015625\n",
            "loss/total -0.01318359375\n",
            "loss/total -0.02734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.049560546875\n",
            "loss/total 0.1630859375\n",
            "loss/total 0.68359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.0302734375\n",
            "loss/total -0.001220703125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.044921875\n",
            "loss/total -0.00341796875\n",
            "loss/total 0.107421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.14453125\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.134765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.1083984375\n",
            "loss/total 0.0615234375\n",
            "loss/total 0.035400390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.03515625\n",
            "loss/total -0.0078125\n",
            "loss/total -0.0380859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.046875\n",
            "loss/total -0.007568359375\n",
            "loss/total -0.034423828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.00390625\n",
            "loss/total -0.046875\n",
            "loss/total -0.05712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10546875\n",
            "loss/total 0.0322265625\n",
            "loss/total -0.02001953125\n",
            "loss/total -0.03955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total -0.017578125\n",
            "loss/total -0.048828125\n",
            "loss/total -0.0751953125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.08935546875\n",
            "loss/total 0.0283203125\n",
            "loss/total 0.0087890625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.01806640625\n",
            "loss/total -0.03662109375\n",
            "loss/total -0.05859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.00634765625\n",
            "loss/total -0.02978515625\n",
            "loss/total -0.060791015625\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total -0.03369140625\n",
            "loss/total -0.09375\n",
            "loss/total -0.12158203125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.029296875\n",
            "loss/total -0.03076171875\n",
            "loss/total -0.046875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total -0.01416015625\n",
            "loss/total -0.076171875\n",
            "loss/total -0.10498046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.1142578125\n",
            "loss/total 0.2138671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.064453125\n",
            "loss/total 0.00146484375\n",
            "loss/total -0.03759765625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.027587890625\n",
            "loss/total -0.0029296875\n",
            "loss/total -0.02001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10400390625\n",
            "loss/total -0.038330078125\n",
            "loss/total -0.064453125\n",
            "loss/total -0.078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total -0.005859375\n",
            "loss/total -0.05859375\n",
            "loss/total -0.0791015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.01953125\n",
            "loss/total -0.0234375\n",
            "loss/total -0.02783203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.032958984375\n",
            "loss/total 0.0283203125\n",
            "loss/total 0.00146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total -0.0068359375\n",
            "loss/total 0.03076171875\n",
            "loss/total -0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total -0.01123046875\n",
            "loss/total -0.055908203125\n",
            "loss/total -0.08984375\n",
            "mini-batch training finished\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.052490234375\n",
            "loss/total 0.0341796875\n",
            "loss/total 0.00439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.009765625\n",
            "loss/total -0.021484375\n",
            "loss/total -0.06640625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.012939453125\n",
            "loss/total 0.0009765625\n",
            "loss/total -0.03564453125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total -0.07080078125\n",
            "loss/total -0.125\n",
            "loss/total -0.125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total -0.00732421875\n",
            "loss/total -0.0869140625\n",
            "loss/total -0.1181640625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.029296875\n",
            "loss/total 0.00146484375\n",
            "loss/total -0.029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.064453125\n",
            "loss/total 0.0029296875\n",
            "loss/total -0.0107421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.06396484375\n",
            "loss/total 0.027099609375\n",
            "loss/total 0.00537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total -0.00537109375\n",
            "loss/total -0.0048828125\n",
            "loss/total -0.02294921875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0205078125\n",
            "loss/total -0.01513671875\n",
            "loss/total -0.030517578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.17578125\n",
            "loss/total 0.0751953125\n",
            "loss/total -0.02490234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07958984375\n",
            "loss/total 0.02685546875\n",
            "loss/total -0.00341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.083984375\n",
            "loss/total 0.095703125\n",
            "loss/total 0.047119140625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.044677734375\n",
            "loss/total -0.06982421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.060791015625\n",
            "loss/total -0.01318359375\n",
            "loss/total -0.0205078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10400390625\n",
            "loss/total 0.040771484375\n",
            "loss/total -0.0009765625\n",
            "loss/total -0.01513671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10595703125\n",
            "loss/total -0.02197265625\n",
            "loss/total -0.07666015625\n",
            "loss/total -0.08837890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.0263671875\n",
            "loss/total -0.01416015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.042236328125\n",
            "loss/total -0.0390625\n",
            "loss/total -0.052734375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0107421875\n",
            "loss/total -0.023193359375\n",
            "loss/total -0.046630859375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04541015625\n",
            "loss/total 0.02099609375\n",
            "loss/total 0.0009765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06640625\n",
            "loss/total 0.02294921875\n",
            "loss/total 0.013671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.027587890625\n",
            "loss/total 0.00048828125\n",
            "loss/total -0.0126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.0625\n",
            "loss/total 0.012451171875\n",
            "loss/total -0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.000732421875\n",
            "loss/total -0.009521484375\n",
            "loss/total -0.0576171875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0185546875\n",
            "loss/total -0.02294921875\n",
            "loss/total -0.0361328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.016845703125\n",
            "loss/total -0.01416015625\n",
            "loss/total -0.03173828125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.008544921875\n",
            "loss/total -0.019775390625\n",
            "loss/total -0.0294189453125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.049072265625\n",
            "loss/total 0.01953125\n",
            "loss/total 0.004150390625\n",
            "mini-batch training finished\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.046630859375\n",
            "loss/total 0.01708984375\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.027587890625\n",
            "loss/total 0.0078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.224609375\n",
            "loss/total 0.1337890625\n",
            "loss/total 0.0341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.06884765625\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.05615234375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.019287109375\n",
            "loss/total -0.018798828125\n",
            "loss/total -0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.059814453125\n",
            "loss/total 0.02001953125\n",
            "loss/total 0.013916015625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.03173828125\n",
            "loss/total 0.015625\n",
            "loss/total 0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.058837890625\n",
            "loss/total 0.10400390625\n",
            "loss/total 0.0673828125\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.10546875\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.0537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.03955078125\n",
            "loss/total 0.05712890625\n",
            "loss/total 0.1015625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0517578125\n",
            "loss/total 0.0380859375\n",
            "loss/total 0.023193359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.0185546875\n",
            "loss/total -0.002685546875\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.052001953125\n",
            "loss/total 0.039306640625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.083984375\n",
            "loss/total 0.08544921875\n",
            "loss/total 0.0693359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.08154296875\n",
            "loss/total 0.064453125\n",
            "loss/total 0.057373046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.07421875\n",
            "loss/total 0.055419921875\n",
            "loss/total 0.03759765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.12060546875\n",
            "loss/total 0.0400390625\n",
            "loss/total 0.00537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0576171875\n",
            "loss/total 0.02880859375\n",
            "loss/total 0.0634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.021728515625\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.048583984375\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.035888671875\n",
            "loss/total 0.012451171875\n",
            "loss/total -0.008056640625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.036376953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.0311279296875\n",
            "loss/total 0.02099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.052490234375\n",
            "loss/total 0.037841796875\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.0908203125\n",
            "loss/total 0.107421875\n",
            "loss/total 0.095703125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03662109375\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.1259765625\n",
            "loss/total 0.0830078125\n",
            "loss/total 0.0576171875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.04833984375\n",
            "loss/total 0.041748046875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.08837890625\n",
            "loss/total 0.083984375\n",
            "loss/total 0.07666015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.08203125\n",
            "loss/total 0.046630859375\n",
            "loss/total 0.015380859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0556640625\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.02783203125\n",
            "mini-batch training finished\n",
            "loss/total 0.103515625\n",
            "loss/total 0.095703125\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.056640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.07421875\n",
            "loss/total 0.05859375\n",
            "loss/total 0.05029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.080078125\n",
            "loss/total 0.059814453125\n",
            "loss/total 0.043212890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.04931640625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.037109375\n",
            "loss/total 0.016845703125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.04443359375\n",
            "loss/total -0.00341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0244140625\n",
            "loss/total -0.021484375\n",
            "loss/total -0.052001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.03271484375\n",
            "loss/total 0.0146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0625\n",
            "loss/total 0.02783203125\n",
            "loss/total 0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.0947265625\n",
            "loss/total 0.07177734375\n",
            "loss/total 0.05078125\n",
            "loss/total 0.0390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.060302734375\n",
            "loss/total 0.04345703125\n",
            "loss/total 0.036376953125\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.08203125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0439453125\n",
            "loss/total 0.026611328125\n",
            "loss/total 0.012939453125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.076171875\n",
            "loss/total 0.026611328125\n",
            "loss/total 0.0390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0576171875\n",
            "loss/total 0.0224609375\n",
            "loss/total 0.018310546875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.060791015625\n",
            "loss/total 0.040283203125\n",
            "loss/total 0.026123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0517578125\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.009765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.047607421875\n",
            "loss/total 0.051025390625\n",
            "loss/total 0.02197265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.024658203125\n",
            "loss/total -0.009765625\n",
            "loss/total -0.02587890625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.04638671875\n",
            "loss/total 0.02197265625\n",
            "loss/total -0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.078125\n",
            "loss/total 0.072265625\n",
            "loss/total 0.0634765625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.07373046875\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.0390625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.033203125\n",
            "loss/total 0.0126953125\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.049072265625\n",
            "loss/total 0.01220703125\n",
            "loss/total -0.0048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06689453125\n",
            "loss/total 0.041015625\n",
            "loss/total 0.031005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.02783203125\n",
            "mini-batch training finished\n",
            "loss/total 0.10546875\n",
            "loss/total 0.087890625\n",
            "loss/total 0.05908203125\n",
            "loss/total 0.046630859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.054443359375\n",
            "loss/total 0.03515625\n",
            "loss/total 0.019775390625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.05810546875\n",
            "loss/total 0.050048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.1171875\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.046142578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.028076171875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.057861328125\n",
            "loss/total 0.02685546875\n",
            "loss/total 0.01953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.059326171875\n",
            "loss/total 0.03173828125\n",
            "loss/total 0.01416015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.00732421875\n",
            "loss/total -0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.019287109375\n",
            "loss/total 0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.04248046875\n",
            "loss/total 0.0244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.019287109375\n",
            "loss/total -0.00439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.04248046875\n",
            "loss/total 0.0\n",
            "loss/total -0.03369140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10400390625\n",
            "loss/total 0.0830078125\n",
            "loss/total 0.020263671875\n",
            "loss/total -0.02001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.001953125\n",
            "loss/total -0.03857421875\n",
            "loss/total -0.0546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.08154296875\n",
            "loss/total 0.05224609375\n",
            "loss/total 0.0302734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.034423828125\n",
            "loss/total 0.01171875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.054931640625\n",
            "loss/total 0.02880859375\n",
            "loss/total 0.01025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06640625\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.0244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.06103515625\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.012451171875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.06396484375\n",
            "loss/total 0.0400390625\n",
            "loss/total 0.018310546875\n",
            "mini-batch training finished\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.060546875\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.083984375\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.046142578125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.050048828125\n",
            "loss/total 0.040283203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.064453125\n",
            "loss/total 0.02734375\n",
            "loss/total 0.020263671875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.068359375\n",
            "loss/total 0.046875\n",
            "loss/total 0.03271484375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.04296875\n",
            "loss/total 0.0272216796875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05859375\n",
            "loss/total 0.0380859375\n",
            "loss/total 0.017822265625\n",
            "mini-batch training finished\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.0703125\n",
            "loss/total 0.044189453125\n",
            "loss/total 0.0225830078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.056640625\n",
            "loss/total 0.0361328125\n",
            "loss/total 0.018798828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.04638671875\n",
            "loss/total 0.0233154296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.078125\n",
            "loss/total 0.05224609375\n",
            "loss/total 0.03759765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.05615234375\n",
            "loss/total 0.04541015625\n",
            "loss/total 0.029541015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.030517578125\n",
            "loss/total -0.00439453125\n",
            "loss/total -0.035400390625\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.058837890625\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.02490234375\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.076171875\n",
            "loss/total 0.0537109375\n",
            "loss/total 0.035400390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.040771484375\n",
            "loss/total 0.02294921875\n",
            "loss/total -0.0166015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.033203125\n",
            "loss/total 0.00244140625\n",
            "loss/total -0.02734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.018310546875\n",
            "loss/total -0.03173828125\n",
            "loss/total -0.056640625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.03759765625\n",
            "loss/total 0.021484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.030517578125\n",
            "loss/total -0.01318359375\n",
            "loss/total -0.03857421875\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.0478515625\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.017578125\n",
            "loss/total -0.05859375\n",
            "loss/total -0.0673828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07275390625\n",
            "loss/total 0.039794921875\n",
            "loss/total 0.025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.047607421875\n",
            "loss/total 0.02001953125\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.08642578125\n",
            "loss/total 0.05224609375\n",
            "loss/total 0.019287109375\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.02392578125\n",
            "loss/total -0.007568359375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.064453125\n",
            "loss/total 0.044189453125\n",
            "loss/total 0.029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.032958984375\n",
            "loss/total 0.0517578125\n",
            "loss/total 0.0166015625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.037109375\n",
            "loss/total 0.003173828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.061767578125\n",
            "loss/total 0.033447265625\n",
            "loss/total -0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06884765625\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.03173828125\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.072265625\n",
            "loss/total 0.0546875\n",
            "loss/total 0.03173828125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.0703125\n",
            "loss/total 0.050537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.07275390625\n",
            "loss/total 0.055419921875\n",
            "loss/total 0.036376953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.046142578125\n",
            "loss/total 0.013427734375\n",
            "loss/total -0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.076171875\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.01611328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.04296875\n",
            "loss/total 0.010498046875\n",
            "loss/total -0.01025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.048583984375\n",
            "loss/total 0.021728515625\n",
            "loss/total -0.00537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.05712890625\n",
            "loss/total 0.037841796875\n",
            "loss/total 0.0078125\n",
            "mini-batch training finished\n",
            "loss/total 0.103515625\n",
            "loss/total 0.08837890625\n",
            "loss/total 0.060791015625\n",
            "loss/total 0.0546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.01708984375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.07275390625\n",
            "loss/total 0.05810546875\n",
            "loss/total 0.042236328125\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.015625\n",
            "loss/total -0.0087890625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.068359375\n",
            "loss/total 0.055908203125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.07958984375\n",
            "loss/total 0.06201171875\n",
            "loss/total 0.047119140625\n",
            "mini-batch training finished\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.051025390625\n",
            "loss/total 0.024658203125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06201171875\n",
            "loss/total 0.033203125\n",
            "loss/total 0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.054931640625\n",
            "loss/total 0.03369140625\n",
            "loss/total 0.01025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.053955078125\n",
            "loss/total 0.030029296875\n",
            "loss/total 0.015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.056640625\n",
            "loss/total 0.03857421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1064453125\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.015380859375\n",
            "loss/total 0.0146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.032470703125\n",
            "loss/total -0.01708984375\n",
            "loss/total -0.03955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1064453125\n",
            "loss/total 0.07373046875\n",
            "loss/total 0.060546875\n",
            "loss/total 0.041015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.046875\n",
            "loss/total 0.044189453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.060546875\n",
            "loss/total 0.02978515625\n",
            "loss/total 0.0185546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.038330078125\n",
            "loss/total 0.024658203125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06689453125\n",
            "loss/total 0.029541015625\n",
            "loss/total 0.00390625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.057861328125\n",
            "loss/total 0.036865234375\n",
            "loss/total 0.01025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.0078125\n",
            "loss/total -0.01611328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.03759765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0703125\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.02294921875\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.064453125\n",
            "loss/total 0.080078125\n",
            "loss/total 0.040771484375\n",
            "mini-batch training finished\n",
            "loss/total 0.103515625\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.046142578125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.060791015625\n",
            "loss/total 0.046630859375\n",
            "loss/total 0.0185546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.025146484375\n",
            "loss/total -0.0185546875\n",
            "loss/total -0.055419921875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.072265625\n",
            "loss/total 0.059326171875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05078125\n",
            "loss/total 0.0166015625\n",
            "loss/total 0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.055908203125\n",
            "loss/total 0.045166015625\n",
            "loss/total 0.03125\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.041015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.0478515625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0888671875\n",
            "loss/total 0.045166015625\n",
            "loss/total 0.018798828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.06982421875\n",
            "loss/total 0.056884765625\n",
            "loss/total 0.0279541015625\n",
            "mini-batch training finished\n",
            "loss/total 0.103515625\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.059326171875\n",
            "loss/total 0.054443359375\n",
            "mini-batch training finished\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.0546875\n",
            "loss/total 0.036865234375\n",
            "loss/total 0.015380859375\n",
            "mini-batch training finished\n",
            "LLM Judge Call failed: 'NoneType' object is not subscriptable\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.04736328125\n",
            "loss/total 0.03076171875\n",
            "loss/total 0.0009765625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.05517578125\n",
            "loss/total 0.037841796875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.1279296875\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.072265625\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.06103515625\n",
            "loss/total 0.0390625\n",
            "loss/total 0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.08203125\n",
            "loss/total 0.0615234375\n",
            "loss/total 0.047607421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.078125\n",
            "loss/total 0.055908203125\n",
            "loss/total 0.037353515625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.052001953125\n",
            "loss/total 0.03369140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0830078125\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.049560546875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0703125\n",
            "loss/total 0.056884765625\n",
            "loss/total 0.016845703125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.043701171875\n",
            "loss/total 0.0126953125\n",
            "loss/total -0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.07177734375\n",
            "loss/total 0.0458984375\n",
            "loss/total 0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.248046875\n",
            "loss/total 0.85546875\n",
            "loss/total 0.25390625\n",
            "mini-batch training finished\n",
            "loss/total 0.11083984375\n",
            "loss/total 0.1015625\n",
            "loss/total 0.095703125\n",
            "loss/total 0.08642578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.08984375\n",
            "loss/total 0.060546875\n",
            "loss/total 0.04931640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.06103515625\n",
            "loss/total 0.0301513671875\n",
            "loss/total 0.001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.08447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.046142578125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.076171875\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.052978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.068359375\n",
            "loss/total 0.05126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.072265625\n",
            "loss/total 0.056640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.064453125\n",
            "loss/total 0.04833984375\n",
            "loss/total 0.0211181640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.044921875\n",
            "loss/total 0.022705078125\n",
            "loss/total 0.00439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.103515625\n",
            "loss/total 0.0859375\n",
            "loss/total 0.068359375\n",
            "loss/total 0.048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.0517578125\n",
            "loss/total 0.0458984375\n",
            "mini-batch training finished\n",
            "loss/total 0.1123046875\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.0703125\n",
            "mini-batch training finished\n",
            "loss/total 0.103515625\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.0595703125\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.07763671875\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.06689453125\n",
            "loss/total 0.044189453125\n",
            "loss/total 0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.1083984375\n",
            "loss/total 0.08837890625\n",
            "loss/total 0.083984375\n",
            "loss/total 0.068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.055419921875\n",
            "loss/total 0.02490234375\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.064453125\n",
            "loss/total 0.033447265625\n",
            "loss/total 0.014892578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.05712890625\n",
            "loss/total 0.033203125\n",
            "loss/total 0.00439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.04638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.04150390625\n",
            "loss/total 0.0224609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.053466796875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.08984375\n",
            "loss/total 0.07861328125\n",
            "loss/total 0.0673828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.0322265625\n",
            "loss/total 0.020263671875\n",
            "mini-batch training finished\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.058837890625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09375\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.07080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.08056640625\n",
            "loss/total 0.072265625\n",
            "loss/total 0.05078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.035400390625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.052734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.08447265625\n",
            "loss/total 0.08154296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10546875\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.0595703125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0625\n",
            "loss/total 0.050048828125\n",
            "loss/total 0.043212890625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.07861328125\n",
            "loss/total 0.068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.080078125\n",
            "loss/total 0.078125\n",
            "loss/total 0.0703125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0830078125\n",
            "loss/total 0.0537109375\n",
            "loss/total 0.02978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.080078125\n",
            "loss/total 0.06640625\n",
            "loss/total 0.05224609375\n",
            "mini-batch training finished\n",
            "loss/total 0.109375\n",
            "loss/total 0.08154296875\n",
            "loss/total 0.056884765625\n",
            "loss/total 0.0181884765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.061767578125\n",
            "loss/total 0.038818359375\n",
            "loss/total 0.02734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.03173828125\n",
            "loss/total 0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.059814453125\n",
            "loss/total 0.041015625\n",
            "loss/total 0.0283203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.06640625\n",
            "loss/total 0.03759765625\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.08251953125\n",
            "loss/total 0.0751953125\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.08984375\n",
            "loss/total 0.0751953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.033203125\n",
            "loss/total 0.01953125\n",
            "loss/total 0.00341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.103515625\n",
            "loss/total 0.0888671875\n",
            "loss/total 0.08447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.052734375\n",
            "loss/total 0.036376953125\n",
            "loss/total 0.021484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.045654296875\n",
            "loss/total 0.0166015625\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.08251953125\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.056640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0869140625\n",
            "mini-batch training finished\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.046630859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.07958984375\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.057861328125\n",
            "mini-batch training finished\n",
            "loss/total 0.10693359375\n",
            "loss/total 0.07421875\n",
            "loss/total 0.0537109375\n",
            "loss/total 0.041015625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.053955078125\n",
            "loss/total 0.029052734375\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.058349609375\n",
            "loss/total 0.031982421875\n",
            "loss/total 0.006103515625\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.046142578125\n",
            "loss/total 0.0068359375\n",
            "loss/total -0.02392578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.08154296875\n",
            "loss/total 0.059326171875\n",
            "loss/total 0.039306640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.039794921875\n",
            "loss/total 0.013671875\n",
            "loss/total -0.00732421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.055908203125\n",
            "loss/total 0.020751953125\n",
            "loss/total 0.005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.01416015625\n",
            "loss/total -0.0205078125\n",
            "loss/total -0.048095703125\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0576171875\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.0380859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.08154296875\n",
            "loss/total 0.05712890625\n",
            "loss/total 0.046142578125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.091796875\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.03515625\n",
            "loss/total 0.005126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.05078125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05810546875\n",
            "loss/total 0.040283203125\n",
            "loss/total 0.023681640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.027587890625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0537109375\n",
            "loss/total 0.041259765625\n",
            "loss/total 0.0283203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.08837890625\n",
            "loss/total 0.06201171875\n",
            "loss/total 0.049072265625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.045654296875\n",
            "loss/total 0.044189453125\n",
            "loss/total 0.01806640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.056396484375\n",
            "loss/total 0.033203125\n",
            "loss/total 0.013671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07373046875\n",
            "loss/total 0.055908203125\n",
            "loss/total 0.037109375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.046875\n",
            "loss/total 0.033203125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.056396484375\n",
            "loss/total 0.03955078125\n",
            "loss/total 0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.02783203125\n",
            "loss/total 0.0068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.05615234375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.091796875\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.072265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0703125\n",
            "loss/total 0.041259765625\n",
            "loss/total 0.024169921875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.040283203125\n",
            "loss/total 0.02294921875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.053955078125\n",
            "loss/total 0.040283203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.044189453125\n",
            "loss/total 0.026123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.057861328125\n",
            "loss/total 0.04443359375\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.09765625\n",
            "loss/total 0.08984375\n",
            "loss/total 0.0830078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.08056640625\n",
            "loss/total 0.056396484375\n",
            "loss/total 0.03857421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0244140625\n",
            "loss/total -0.00732421875\n",
            "loss/total -0.02490234375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07373046875\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.04736328125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0859375\n",
            "loss/total 0.07421875\n",
            "loss/total 0.05908203125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.045166015625\n",
            "loss/total 0.02490234375\n",
            "loss/total 0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.047119140625\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.080078125\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.05712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.1083984375\n",
            "loss/total 0.08447265625\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.05029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10546875\n",
            "loss/total 0.078125\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.046875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.04248046875\n",
            "loss/total 0.0126953125\n",
            "loss/total 0.00244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.080078125\n",
            "loss/total 0.07470703125\n",
            "loss/total 0.064453125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.023681640625\n",
            "loss/total 0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.0625\n",
            "loss/total 0.046875\n",
            "loss/total 0.0234375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.03125\n",
            "loss/total 0.01318359375\n",
            "loss/total -0.017578125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.09326171875\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.07177734375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.05615234375\n",
            "loss/total 0.019775390625\n",
            "loss/total -0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.033447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.01806640625\n",
            "loss/total 0.005859375\n",
            "loss/total -0.0107421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07470703125\n",
            "loss/total 0.05908203125\n",
            "loss/total 0.041259765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.048095703125\n",
            "loss/total 0.0322265625\n",
            "loss/total 0.015869140625\n",
            "mini-batch training finished\n",
            "loss/total 0.1083984375\n",
            "loss/total 0.08984375\n",
            "loss/total 0.07177734375\n",
            "loss/total 0.05517578125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.057861328125\n",
            "loss/total 0.04736328125\n",
            "loss/total 0.033935546875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.083984375\n",
            "loss/total 0.07080078125\n",
            "loss/total 0.05419921875\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.01416015625\n",
            "loss/total -0.02001953125\n",
            "loss/total -0.0478515625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.055908203125\n",
            "loss/total 0.039794921875\n",
            "loss/total 0.023193359375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.08935546875\n",
            "loss/total 0.080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.0703125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.080078125\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.046142578125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.08056640625\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.04638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.08642578125\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.052490234375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.07470703125\n",
            "loss/total 0.060546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06982421875\n",
            "loss/total 0.036376953125\n",
            "loss/total 0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.08837890625\n",
            "loss/total 0.08154296875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.091796875\n",
            "loss/total 0.09375\n",
            "loss/total 0.0927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.054931640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.06396484375\n",
            "loss/total 0.040771484375\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.04052734375\n",
            "loss/total 0.014892578125\n",
            "loss/total -0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.060302734375\n",
            "loss/total 0.037109375\n",
            "loss/total 0.0291748046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.041259765625\n",
            "loss/total 0.0185546875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.08935546875\n",
            "loss/total 0.07421875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0703125\n",
            "loss/total -0.0107421875\n",
            "loss/total -0.04443359375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.05712890625\n",
            "loss/total 0.050048828125\n",
            "loss/total 0.03955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.09765625\n",
            "loss/total 0.08935546875\n",
            "loss/total 0.0771484375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0947265625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.03466796875\n",
            "loss/total -0.010498046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10400390625\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.0908203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0615234375\n",
            "loss/total 0.033203125\n",
            "loss/total 0.028076171875\n",
            "mini-batch training finished\n",
            "loss/total 0.1083984375\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.0859375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.06591796875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.045166015625\n",
            "loss/total 0.0224609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.09375\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07861328125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.05810546875\n",
            "loss/total 0.04443359375\n",
            "loss/total 0.0302734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.05712890625\n",
            "loss/total 0.031005859375\n",
            "loss/total 0.01416015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.04296875\n",
            "loss/total 0.033203125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.08642578125\n",
            "loss/total 0.07421875\n",
            "loss/total 0.05908203125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.1708984375\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.0888671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.08837890625\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.061767578125\n",
            "loss/total 0.053955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.056396484375\n",
            "loss/total 0.02880859375\n",
            "loss/total 0.0048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.08251953125\n",
            "loss/total 0.06689453125\n",
            "loss/total 0.052734375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.06103515625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0888671875\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.06640625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.087890625\n",
            "loss/total 0.07861328125\n",
            "loss/total 0.060791015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.050537109375\n",
            "loss/total 0.02587890625\n",
            "loss/total 0.011962890625\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.06689453125\n",
            "loss/total 0.054443359375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0830078125\n",
            "loss/total 0.0712890625\n",
            "loss/total 0.05712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.078125\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.0576171875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.041015625\n",
            "loss/total 0.029296875\n",
            "loss/total 0.0126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.05908203125\n",
            "loss/total 0.0341796875\n",
            "loss/total 0.010986328125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.021240234375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.03515625\n",
            "loss/total 0.00390625\n",
            "loss/total -0.022705078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10498046875\n",
            "loss/total 0.08935546875\n",
            "loss/total 0.07861328125\n",
            "loss/total 0.07080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.08544921875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.052734375\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.080078125\n",
            "loss/total 0.060791015625\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.06884765625\n",
            "loss/total 0.05029296875\n",
            "loss/total 0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.0302734375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.044921875\n",
            "loss/total 0.01025390625\n",
            "loss/total -0.017822265625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.05322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.056640625\n",
            "loss/total 0.037109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.054443359375\n",
            "loss/total 0.036376953125\n",
            "loss/total 0.021728515625\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.03662109375\n",
            "loss/total 0.01171875\n",
            "loss/total -0.018798828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.048095703125\n",
            "loss/total 0.031494140625\n",
            "loss/total 0.02099609375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.080078125\n",
            "loss/total 0.061767578125\n",
            "loss/total 0.052978515625\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.0859375\n",
            "loss/total 0.056640625\n",
            "loss/total 0.0380859375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.064453125\n",
            "loss/total 0.047607421875\n",
            "loss/total 0.029541015625\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.0810546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10498046875\n",
            "loss/total 0.091796875\n",
            "loss/total 0.072265625\n",
            "loss/total 0.06298828125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.068359375\n",
            "loss/total 0.060302734375\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.05810546875\n",
            "loss/total 0.039306640625\n",
            "loss/total 0.021728515625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.060546875\n",
            "loss/total 0.039794921875\n",
            "loss/total 0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.09375\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0791015625\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.078125\n",
            "loss/total 0.05517578125\n",
            "loss/total 0.0458984375\n",
            "mini-batch training finished\n",
            "loss/total 0.10400390625\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.01904296875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.0703125\n",
            "loss/total 0.0537109375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.02880859375\n",
            "loss/total 0.00244140625\n",
            "loss/total -0.015869140625\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.08203125\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.06103515625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.05224609375\n",
            "loss/total 0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.05419921875\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.013671875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.09375\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.07861328125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.07421875\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.01025390625\n",
            "loss/total -0.01513671875\n",
            "mini-batch training finished\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.04150390625\n",
            "loss/total 0.02392578125\n",
            "loss/total 0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.039306640625\n",
            "loss/total 0.021484375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.09765625\n",
            "loss/total 0.08642578125\n",
            "loss/total 0.07421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.046630859375\n",
            "loss/total 0.035888671875\n",
            "loss/total 0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.08154296875\n",
            "loss/total 0.0712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.039306640625\n",
            "loss/total 0.02197265625\n",
            "loss/total 0.00830078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.09326171875\n",
            "loss/total 0.0888671875\n",
            "loss/total 0.07861328125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07958984375\n",
            "loss/total 0.06298828125\n",
            "loss/total 0.0439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.05517578125\n",
            "loss/total 0.047119140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07080078125\n",
            "loss/total 0.049072265625\n",
            "loss/total 0.033447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.087890625\n",
            "loss/total 0.08447265625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.03173828125\n",
            "loss/total 0.02294921875\n",
            "loss/total 0.0029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0859375\n",
            "loss/total 0.0810546875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.10595703125\n",
            "loss/total 0.09375\n",
            "loss/total 0.0849609375\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.087890625\n",
            "loss/total 0.061279296875\n",
            "loss/total 0.039794921875\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.0703125\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.0303955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.052734375\n",
            "loss/total 0.03076171875\n",
            "loss/total 0.0068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.09375\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.013916015625\n",
            "loss/total -0.00439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.031494140625\n",
            "loss/total -0.00244140625\n",
            "loss/total -0.0263671875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.060302734375\n",
            "loss/total 0.0341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.09765625\n",
            "loss/total 0.09228515625\n",
            "loss/total 0.0673828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.068359375\n",
            "loss/total 0.0595703125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.048828125\n",
            "loss/total 0.0048828125\n",
            "loss/total -0.026123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.046142578125\n",
            "loss/total 0.026123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.07666015625\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.05029296875\n",
            "mini-batch training finished\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.06982421875\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.02294921875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.10986328125\n",
            "loss/total 0.07177734375\n",
            "loss/total 0.06884765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.057373046875\n",
            "loss/total 0.0400390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.047607421875\n",
            "loss/total 0.0224609375\n",
            "loss/total 0.01025390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06201171875\n",
            "loss/total 0.042724609375\n",
            "loss/total 0.01318359375\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.031982421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0595703125\n",
            "loss/total 0.030029296875\n",
            "loss/total 0.01171875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.0498046875\n",
            "loss/total 0.03369140625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.07421875\n",
            "loss/total 0.064453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.05859375\n",
            "loss/total 0.051513671875\n",
            "loss/total 0.032958984375\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.10302734375\n",
            "loss/total 0.09326171875\n",
            "loss/total 0.076171875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.03173828125\n",
            "loss/total -0.01171875\n",
            "loss/total -0.04150390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.09765625\n",
            "loss/total 0.076171875\n",
            "loss/total 0.0654296875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.00830078125\n",
            "loss/total -0.0126953125\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.1171875\n",
            "loss/total 0.0615234375\n",
            "loss/total 0.057861328125\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.083984375\n",
            "loss/total 0.0712890625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.06640625\n",
            "mini-batch training finished\n",
            "loss/total 0.09619140625\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.05517578125\n",
            "loss/total 0.03759765625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0556640625\n",
            "loss/total 0.04638671875\n",
            "loss/total 0.032958984375\n",
            "mini-batch training finished\n",
            "loss/total 0.0947265625\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.0693359375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.0849609375\n",
            "loss/total 0.0771484375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.046142578125\n",
            "loss/total 0.02490234375\n",
            "loss/total -0.00146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.02490234375\n",
            "loss/total -0.0029296875\n",
            "loss/total -0.02783203125\n",
            "mini-batch training finished\n",
            "loss/total 0.0947265625\n",
            "loss/total 0.0419921875\n",
            "loss/total 0.00341796875\n",
            "loss/total -0.01806640625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0771484375\n",
            "loss/total 0.04833984375\n",
            "loss/total 0.03564453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.060302734375\n",
            "loss/total 0.028564453125\n",
            "loss/total 0.01123046875\n",
            "mini-batch training finished\n",
            "loss/total 0.095703125\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.0478515625\n",
            "loss/total 0.0303955078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.055419921875\n",
            "loss/total 0.042724609375\n",
            "loss/total 0.0341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.0546875\n",
            "loss/total 0.034423828125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.07421875\n",
            "loss/total 0.0615234375\n",
            "loss/total 0.064453125\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.078125\n",
            "loss/total 0.0615234375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.053955078125\n",
            "loss/total 0.041259765625\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.078125\n",
            "loss/total 0.02392578125\n",
            "loss/total -0.001953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.09130859375\n",
            "loss/total 0.06787109375\n",
            "loss/total 0.05419921875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.043212890625\n",
            "loss/total 0.01123046875\n",
            "loss/total -0.005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.064453125\n",
            "loss/total 0.036376953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.0439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.080078125\n",
            "loss/total 0.037841796875\n",
            "loss/total 0.009765625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07958984375\n",
            "loss/total 0.05908203125\n",
            "loss/total 0.052734375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.06396484375\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.095703125\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.0751953125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.08935546875\n",
            "loss/total 0.080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.08251953125\n",
            "loss/total 0.07080078125\n",
            "loss/total 0.0615234375\n",
            "mini-batch training finished\n",
            "loss/total 0.1064453125\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.08251953125\n",
            "loss/total 0.06982421875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.035400390625\n",
            "loss/total 0.0205078125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.083984375\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.060546875\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.06005859375\n",
            "loss/total 0.051513671875\n",
            "mini-batch training finished\n",
            "loss/total 0.09375\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.07568359375\n",
            "loss/total 0.068359375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0869140625\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.0478515625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.06494140625\n",
            "loss/total 0.04931640625\n",
            "loss/total 0.0322265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.076171875\n",
            "loss/total 0.0595703125\n",
            "loss/total 0.047607421875\n",
            "mini-batch training finished\n",
            "loss/total 0.09765625\n",
            "loss/total 0.05517578125\n",
            "loss/total 0.04296875\n",
            "loss/total -0.003173828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10498046875\n",
            "loss/total 0.048095703125\n",
            "loss/total 0.01806640625\n",
            "loss/total -0.005615234375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.05419921875\n",
            "loss/total 0.020751953125\n",
            "loss/total 0.00146484375\n",
            "mini-batch training finished\n",
            "loss/total 0.107421875\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.064453125\n",
            "loss/total 0.037109375\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0859375\n",
            "loss/total 0.062255859375\n",
            "loss/total 0.04638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.0634765625\n",
            "loss/total 0.0294189453125\n",
            "loss/total 0.0142822265625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.07080078125\n",
            "loss/total 0.0595703125\n",
            "loss/total 0.0279541015625\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.099609375\n",
            "loss/total 0.080078125\n",
            "loss/total 0.07421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.033203125\n",
            "mini-batch training finished\n",
            "loss/total 0.1064453125\n",
            "loss/total 0.0751953125\n",
            "loss/total 0.07177734375\n",
            "loss/total 0.060546875\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.052001953125\n",
            "loss/total 0.038330078125\n",
            "loss/total 0.0244140625\n",
            "mini-batch training finished\n",
            "loss/total 0.10205078125\n",
            "loss/total 0.09521484375\n",
            "loss/total 0.09033203125\n",
            "loss/total 0.08642578125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.083984375\n",
            "mini-batch training finished\n",
            "loss/total 0.09814453125\n",
            "loss/total 0.09423828125\n",
            "loss/total 0.050048828125\n",
            "loss/total 0.02685546875\n",
            "mini-batch training finished\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.08984375\n",
            "loss/total 0.0810546875\n",
            "loss/total 0.0703125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.058837890625\n",
            "loss/total 0.03857421875\n",
            "loss/total 0.0283203125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.0927734375\n",
            "loss/total 0.08447265625\n",
            "loss/total 0.0703125\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.087890625\n",
            "loss/total 0.061279296875\n",
            "loss/total 0.047607421875\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.091796875\n",
            "loss/total 0.0673828125\n",
            "loss/total 0.04638671875\n",
            "mini-batch training finished\n",
            "loss/total 0.1015625\n",
            "loss/total 0.09375\n",
            "loss/total 0.09716796875\n",
            "loss/total 0.07080078125\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0888671875\n",
            "loss/total 0.08544921875\n",
            "mini-batch training finished\n",
            "loss/total 0.10791015625\n",
            "loss/total 0.1015625\n",
            "loss/total 0.0966796875\n",
            "loss/total 0.0927734375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.091796875\n",
            "loss/total 0.060546875\n",
            "loss/total 0.040771484375\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.0654296875\n",
            "loss/total 0.044677734375\n",
            "loss/total 0.01416015625\n",
            "mini-batch training finished\n",
            "loss/total 0.0986328125\n",
            "loss/total 0.07177734375\n",
            "loss/total 0.07763671875\n",
            "loss/total 0.06005859375\n",
            "mini-batch training finished\n",
            "loss/total 0.1044921875\n",
            "loss/total 0.09765625\n",
            "loss/total 0.0859375\n",
            "loss/total 0.064453125\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.06591796875\n",
            "loss/total 0.047607421875\n",
            "loss/total 0.034912109375\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.0693359375\n",
            "loss/total 0.030517578125\n",
            "loss/total 0.00341796875\n",
            "mini-batch training finished\n",
            "loss/total 0.09912109375\n",
            "loss/total 0.058837890625\n",
            "loss/total 0.03515625\n",
            "loss/total 0.00048828125\n",
            "mini-batch training finished\n",
            "loss/total 0.10107421875\n",
            "loss/total 0.08837890625\n",
            "loss/total 0.078125\n",
            "loss/total 0.07275390625\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.08349609375\n",
            "loss/total 0.05322265625\n",
            "loss/total 0.037841796875\n",
            "mini-batch training finished\n",
            "loss/total 0.1025390625\n",
            "loss/total 0.0791015625\n",
            "loss/total 0.0732421875\n",
            "loss/total 0.06982421875\n",
            "mini-batch training finished\n",
            "loss/total 0.1005859375\n",
            "loss/total 0.02294921875\n",
            "loss/total -0.0048828125\n",
            "loss/total -0.032958984375\n",
            "mini-batch training finished\n",
            "loss/total 0.10009765625\n",
            "loss/total 0.04638671875\n",
            "loss/total 0.02001953125\n",
            "loss/total -0.00439453125\n",
            "mini-batch training finished\n",
            "loss/total 0.099609375\n",
            "loss/total 0.09375\n",
            "loss/total 0.08740234375\n",
            "loss/total 0.07763671875\n",
            "mini-batch training finished\n",
            "epoch 1 finished\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Generate responses\n",
        "        query_tensors = batch['input_ids']\n",
        "        query_attention_masks = batch['attention_mask']\n",
        "\n",
        "        response_tensors = []\n",
        "        query_response_tensors = []\n",
        "        score_tensors = []\n",
        "\n",
        "        for i, query in enumerate(query_tensors):\n",
        "            query = query.to(device)\n",
        "            query_attention_mask = query_attention_masks[i].to(device)\n",
        "            new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "            query_response = model.generate(\n",
        "                input_ids=query.unsqueeze(0),\n",
        "                attention_mask=query_attention_mask.unsqueeze(0),\n",
        "                **generation_kwargs\n",
        "                ).squeeze(0)\n",
        "\n",
        "            response_len = len(query_response) - len(query)\n",
        "            response_tensors.append(query_response[-response_len:])\n",
        "            query_response_tensors.append(query_response)\n",
        "\n",
        "            # Use LLM judge instead of reward model\n",
        "            with torch.no_grad():\n",
        "                prompt_text = tokenizer.decode(query, skip_special_tokens=True)\n",
        "                response_text = tokenizer.decode(query_response[-response_len:], skip_special_tokens=True)\n",
        "\n",
        "                # Get score from LLM judge\n",
        "                score = get_llm_judge_score(prompt_text, response_text)\n",
        "                score = torch.tensor(score).to(device)\n",
        "\n",
        "            score_tensors.append(score)\n",
        "\n",
        "        input_data = data_collator([\n",
        "            {\n",
        "                'input_ids': ids,\n",
        "                'attention_mask': torch.ones_like(ids)\n",
        "            }\n",
        "            for ids in query_response_tensors\n",
        "        ]).to(device)\n",
        "\n",
        "        # rewards and advantages\n",
        "        logprobs, rewards, values, masks = compute_rewards(input_data, query_tensors, response_tensors, score_tensors)\n",
        "        advantages, returns = compute_advantage(rewards, values, masks)\n",
        "\n",
        "        # mini batch training\n",
        "        mini_batch_train()\n",
        "    print(f'epoch {epoch + 1} finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar72m6pq7XWv"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "import os\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ppo_model_epoch_1.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p91mWY7ZAMw9"
      },
      "source": [
        "## Validation\n",
        "\n",
        "Test the trained model on the validation set to see if responses improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUEfXiPmOybE"
      },
      "outputs": [],
      "source": [
        "# len(tokenized_dataset_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xdzy5XlO07I"
      },
      "outputs": [],
      "source": [
        "# val_gen_lengths = [0] * len(tokenized_dataset_val)\n",
        "# for i in range(len(tokenized_dataset_val)):\n",
        "#     val_gen_lengths[i] = random.choice(list(range(output_min_length, output_max_length)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPM6f46xPyxO"
      },
      "outputs": [],
      "source": [
        "# val_gen_lengths[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y08Nx89CP2je"
      },
      "outputs": [],
      "source": [
        "# def validate():\n",
        "#     scores = []\n",
        "#     for b, batch in enumerate(val_dataloader):\n",
        "#         # Generate_responses\n",
        "#         query_tensors = batch['input_ids']\n",
        "#         query_attention_masks = batch['attention_mask']\n",
        "#         for i, query in enumerate(query_tensors):\n",
        "#             query = query.to(device)\n",
        "#             query_attention_mask = query_attention_masks[i].to(device)\n",
        "#             new_tokens = val_gen_lengths[b * len(query_tensors) + i]\n",
        "#             generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "#             query_response = model.generate(\n",
        "#                 input_ids=query.unsqueeze(0),\n",
        "#                 attention_mask=query_attention_mask.unsqueeze(0),\n",
        "#                 **generation_kwargs\n",
        "#                 ).squeeze(0)\n",
        "#             # query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
        "#             attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
        "#             score = reward_model(query_response_score.unsqueeze(0), attention_mask.unsqueeze(0)).squeeze(0)[-1]\n",
        "#             score = 2 * (score - 0.5)\n",
        "#             scores.append(score.item())\n",
        "#     print('avg score:', sum(scores) / len(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvD7FCQ3SF7K"
      },
      "outputs": [],
      "source": [
        "# validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOnXVMV7TaRy"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'ppo_model_epoch_1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwf0VWEuSG0G"
      },
      "outputs": [],
      "source": [
        "# model_path = './sft_model_epoch_1'\n",
        "# model = ModelForCausalLMWithValueHead(model_path).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40GywjqISiAW"
      },
      "outputs": [],
      "source": [
        "# validate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03e4b5e63f65430a9f8b40036712d9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07219d82508149a68e92db965b3f54fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7506c22b41bd4f34b270d04a0733c9c7",
              "IPY_MODEL_ce05388fd7184f04a46443845342588d",
              "IPY_MODEL_58e7377bfbdc44e2bd702ec9377eed41"
            ],
            "layout": "IPY_MODEL_393e7ec9512744698c9e3f069d152419"
          }
        },
        "098155374c7f43fcbf13101484501f60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c5675f65f24ab3b7a7dc2bab6bd346": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a562dbb95c84e6bad1a6fbdb166a553": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a59e6f68884969904d6d75a1f03f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "360ec0546e20467d824dbe25f9309de4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393e7ec9512744698c9e3f069d152419": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef1c5ad46304c9a8a2efaef0ab3d3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098155374c7f43fcbf13101484501f60",
            "placeholder": "​",
            "style": "IPY_MODEL_6ae6d60c69b34f8c846ce2643285ceb4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "41b8f0c1f75045e7b829e1be2847a9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a562dbb95c84e6bad1a6fbdb166a553",
            "max": 1800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26a59e6f68884969904d6d75a1f03f1b",
            "value": 1800
          }
        },
        "44a76570ee3348a5acfb4c9ec54e35af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46bd263ea7664b3db23d5e0439b3339a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b0031ba1cbb429a9a2d042fe8848d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534eb88651444ca0927356c6555c2c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e7377bfbdc44e2bd702ec9377eed41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_360ec0546e20467d824dbe25f9309de4",
            "placeholder": "​",
            "style": "IPY_MODEL_f36b916f761d48c3809a78ebe17abd1f",
            "value": " 200/200 [00:00&lt;00:00, 6217.93 examples/s]"
          }
        },
        "59cc4a99088c4e5fa73f0c5b9988c245": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "662f7f2b3c284365af7e38e415892a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6793babb319c4164b24a565ac7f9e6a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae6d60c69b34f8c846ce2643285ceb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d30abd474164a43863deed3dbef754e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7506c22b41bd4f34b270d04a0733c9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b00a8f3ee230498294d2a3420e15b651",
            "placeholder": "​",
            "style": "IPY_MODEL_03e4b5e63f65430a9f8b40036712d9e6",
            "value": "Filter: 100%"
          }
        },
        "7afdf763b19749f7a9fcae9d46194c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc325f7c485d4c8cba4aadc4f508cff2",
            "placeholder": "​",
            "style": "IPY_MODEL_44a76570ee3348a5acfb4c9ec54e35af",
            "value": "Filter: 100%"
          }
        },
        "7d2f7bfe2a36424691182d03cc90bdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46bd263ea7664b3db23d5e0439b3339a",
            "placeholder": "​",
            "style": "IPY_MODEL_4b0031ba1cbb429a9a2d042fe8848d20",
            "value": " 8/8 [00:16&lt;00:00,  1.72s/it]"
          }
        },
        "882ce8572b1841bcbe1dd2cc97bdcf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ef1c5ad46304c9a8a2efaef0ab3d3ba",
              "IPY_MODEL_9d3924e9571d4a11917509f1b5baffc3",
              "IPY_MODEL_7d2f7bfe2a36424691182d03cc90bdcb"
            ],
            "layout": "IPY_MODEL_662f7f2b3c284365af7e38e415892a7b"
          }
        },
        "9d3924e9571d4a11917509f1b5baffc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8f2918a48c40caacd2c3cb1cea637b",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e19b89980a924b3b8a17c63917268522",
            "value": 8
          }
        },
        "b00a8f3ee230498294d2a3420e15b651": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f4629493784dc7b1f60042c205b967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534eb88651444ca0927356c6555c2c2a",
            "placeholder": "​",
            "style": "IPY_MODEL_6d30abd474164a43863deed3dbef754e",
            "value": " 1800/1800 [00:00&lt;00:00, 7660.97 examples/s]"
          }
        },
        "bd8f2918a48c40caacd2c3cb1cea637b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce05388fd7184f04a46443845342588d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6793babb319c4164b24a565ac7f9e6a8",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59cc4a99088c4e5fa73f0c5b9988c245",
            "value": 200
          }
        },
        "e19b89980a924b3b8a17c63917268522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e236bfcf324a4aabbacfcaba2972166c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7afdf763b19749f7a9fcae9d46194c60",
              "IPY_MODEL_41b8f0c1f75045e7b829e1be2847a9e7",
              "IPY_MODEL_b2f4629493784dc7b1f60042c205b967"
            ],
            "layout": "IPY_MODEL_10c5675f65f24ab3b7a7dc2bab6bd346"
          }
        },
        "f36b916f761d48c3809a78ebe17abd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc325f7c485d4c8cba4aadc4f508cff2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
